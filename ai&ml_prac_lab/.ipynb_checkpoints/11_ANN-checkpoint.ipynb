{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BFKyZKp3-Hjr",
    "outputId": "7bd53661-29db-47c1-ec96-5b3142d79da3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 2s 28ms/step - loss: 10.6755 - accuracy: 0.5839 - val_loss: 4.6581 - val_accuracy: 0.7968\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2.5441 - accuracy: 0.5536 - val_loss: 1.8897 - val_accuracy: 0.7968\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9701 - accuracy: 0.6429 - val_loss: 0.5395 - val_accuracy: 0.8021\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6925 - accuracy: 0.7000 - val_loss: 0.9028 - val_accuracy: 0.7968\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.9967 - accuracy: 0.6464 - val_loss: 0.6646 - val_accuracy: 0.7968\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8735 - accuracy: 0.7071 - val_loss: 0.6690 - val_accuracy: 0.6952\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5795 - accuracy: 0.7482 - val_loss: 0.5209 - val_accuracy: 0.8289\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.7429 - val_loss: 0.5551 - val_accuracy: 0.7968\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7540 - accuracy: 0.7054 - val_loss: 0.7043 - val_accuracy: 0.7968\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5816 - accuracy: 0.7411 - val_loss: 0.5380 - val_accuracy: 0.7968\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6521 - accuracy: 0.7429 - val_loss: 0.9195 - val_accuracy: 0.7968\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7419 - accuracy: 0.7250 - val_loss: 0.6319 - val_accuracy: 0.7968\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6201 - accuracy: 0.7393 - val_loss: 0.4880 - val_accuracy: 0.8289\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6213 - accuracy: 0.7357 - val_loss: 0.7404 - val_accuracy: 0.7968\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.8627 - accuracy: 0.7000 - val_loss: 1.4718 - val_accuracy: 0.7968\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7276 - accuracy: 0.7018 - val_loss: 0.7740 - val_accuracy: 0.7968\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6543 - accuracy: 0.7268 - val_loss: 0.8986 - val_accuracy: 0.7968\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6988 - accuracy: 0.7357 - val_loss: 0.4773 - val_accuracy: 0.8342\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8897 - accuracy: 0.7214 - val_loss: 0.6598 - val_accuracy: 0.7487\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1.2834 - accuracy: 0.6536 - val_loss: 2.0821 - val_accuracy: 0.7968\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.4397 - accuracy: 0.6929 - val_loss: 1.1476 - val_accuracy: 0.5775\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.2795 - accuracy: 0.6625 - val_loss: 1.4705 - val_accuracy: 0.7968\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6766 - accuracy: 0.7339 - val_loss: 0.4590 - val_accuracy: 0.8396\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6043 - accuracy: 0.7393 - val_loss: 0.4515 - val_accuracy: 0.7968\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5550 - accuracy: 0.7482 - val_loss: 0.5080 - val_accuracy: 0.7968\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6658 - accuracy: 0.7232 - val_loss: 1.2359 - val_accuracy: 0.7968\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.7233 - accuracy: 0.6554 - val_loss: 1.1919 - val_accuracy: 0.5882\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6969 - accuracy: 0.7339 - val_loss: 0.9972 - val_accuracy: 0.6631\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7907 - accuracy: 0.7000 - val_loss: 0.5162 - val_accuracy: 0.7968\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5888 - accuracy: 0.7482 - val_loss: 0.6415 - val_accuracy: 0.7968\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7306 - accuracy: 0.7179 - val_loss: 0.4717 - val_accuracy: 0.8128\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6236 - accuracy: 0.7411 - val_loss: 0.4560 - val_accuracy: 0.8021\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5768 - accuracy: 0.7411 - val_loss: 0.4684 - val_accuracy: 0.8182\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5994 - accuracy: 0.7411 - val_loss: 0.6688 - val_accuracy: 0.7968\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5836 - accuracy: 0.7411 - val_loss: 0.6195 - val_accuracy: 0.7754\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6167 - accuracy: 0.7304 - val_loss: 0.6266 - val_accuracy: 0.7701\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7088 - accuracy: 0.7286 - val_loss: 0.5926 - val_accuracy: 0.8021\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7989 - accuracy: 0.7036 - val_loss: 1.4093 - val_accuracy: 0.5348\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8582 - accuracy: 0.7036 - val_loss: 0.5257 - val_accuracy: 0.8128\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6032 - accuracy: 0.7518 - val_loss: 0.5209 - val_accuracy: 0.7968\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6231 - accuracy: 0.7375 - val_loss: 0.5595 - val_accuracy: 0.7968\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9034 - accuracy: 0.7054 - val_loss: 1.1983 - val_accuracy: 0.7968\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7222 - accuracy: 0.7232 - val_loss: 0.6052 - val_accuracy: 0.7914\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.0902 - accuracy: 0.7036 - val_loss: 0.7036 - val_accuracy: 0.7540\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.8912 - accuracy: 0.6911 - val_loss: 0.6982 - val_accuracy: 0.7540\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6424 - accuracy: 0.7321 - val_loss: 0.6433 - val_accuracy: 0.7647\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8517 - accuracy: 0.7054 - val_loss: 0.9225 - val_accuracy: 0.7005\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8395 - accuracy: 0.7196 - val_loss: 0.6356 - val_accuracy: 0.7701\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6626 - accuracy: 0.7268 - val_loss: 0.8133 - val_accuracy: 0.7968\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6747 - accuracy: 0.7393 - val_loss: 0.6339 - val_accuracy: 0.7754\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.7446 - val_loss: 0.4462 - val_accuracy: 0.8289\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5609 - accuracy: 0.7607 - val_loss: 0.4503 - val_accuracy: 0.8021\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5677 - accuracy: 0.7375 - val_loss: 0.6641 - val_accuracy: 0.7647\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8813 - accuracy: 0.6946 - val_loss: 0.8254 - val_accuracy: 0.7968\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8872 - accuracy: 0.6946 - val_loss: 0.5198 - val_accuracy: 0.8075\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6115 - accuracy: 0.7268 - val_loss: 0.9580 - val_accuracy: 0.7968\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6381 - accuracy: 0.7321 - val_loss: 0.4365 - val_accuracy: 0.7968\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5801 - accuracy: 0.7643 - val_loss: 0.8304 - val_accuracy: 0.7968\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6702 - accuracy: 0.7357 - val_loss: 0.4839 - val_accuracy: 0.8128\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5654 - accuracy: 0.7446 - val_loss: 0.4521 - val_accuracy: 0.8021\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5561 - accuracy: 0.7446 - val_loss: 0.5131 - val_accuracy: 0.7968\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6118 - accuracy: 0.7357 - val_loss: 0.4773 - val_accuracy: 0.7968\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8382 - accuracy: 0.7321 - val_loss: 0.4942 - val_accuracy: 0.8128\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6273 - accuracy: 0.7518 - val_loss: 0.7772 - val_accuracy: 0.7968\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.9038 - accuracy: 0.6911 - val_loss: 0.6820 - val_accuracy: 0.7647\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6896 - accuracy: 0.7321 - val_loss: 0.4456 - val_accuracy: 0.8235\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6138 - accuracy: 0.7250 - val_loss: 0.9867 - val_accuracy: 0.7968\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7222 - accuracy: 0.7268 - val_loss: 0.5368 - val_accuracy: 0.7968\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5646 - accuracy: 0.7518 - val_loss: 0.4976 - val_accuracy: 0.8128\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7019 - accuracy: 0.7161 - val_loss: 0.5355 - val_accuracy: 0.7968\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6145 - accuracy: 0.7286 - val_loss: 0.4362 - val_accuracy: 0.8235\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5537 - accuracy: 0.7429 - val_loss: 0.7083 - val_accuracy: 0.7647\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7343 - accuracy: 0.7214 - val_loss: 0.4607 - val_accuracy: 0.8182\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5449 - accuracy: 0.7714 - val_loss: 0.5422 - val_accuracy: 0.8075\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6033 - accuracy: 0.7411 - val_loss: 0.7273 - val_accuracy: 0.7968\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6450 - accuracy: 0.7250 - val_loss: 1.5176 - val_accuracy: 0.7968\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.9978 - accuracy: 0.7000 - val_loss: 0.7214 - val_accuracy: 0.7968\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6153 - accuracy: 0.7357 - val_loss: 0.5028 - val_accuracy: 0.7968\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5510 - accuracy: 0.7500 - val_loss: 1.3626 - val_accuracy: 0.7968\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3356 - accuracy: 0.7107 - val_loss: 0.6058 - val_accuracy: 0.7914\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.9113 - accuracy: 0.6839 - val_loss: 1.1926 - val_accuracy: 0.7968\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1707 - accuracy: 0.6946 - val_loss: 0.7686 - val_accuracy: 0.7968\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.7006 - accuracy: 0.7250 - val_loss: 1.3398 - val_accuracy: 0.7968\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.1730 - accuracy: 0.7071 - val_loss: 1.1288 - val_accuracy: 0.6684\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.9839 - accuracy: 0.7018 - val_loss: 1.2081 - val_accuracy: 0.6684\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.9555 - accuracy: 0.6982 - val_loss: 0.4319 - val_accuracy: 0.8075\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.9415 - accuracy: 0.6964 - val_loss: 0.4376 - val_accuracy: 0.8075\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6122 - accuracy: 0.7571 - val_loss: 0.5190 - val_accuracy: 0.8128\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5790 - accuracy: 0.7536 - val_loss: 0.5996 - val_accuracy: 0.8021\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5747 - accuracy: 0.7339 - val_loss: 0.4339 - val_accuracy: 0.8235\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6309 - accuracy: 0.7250 - val_loss: 0.8079 - val_accuracy: 0.7968\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.8255 - accuracy: 0.7214 - val_loss: 0.5888 - val_accuracy: 0.7968\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5623 - accuracy: 0.7518 - val_loss: 0.4411 - val_accuracy: 0.8235\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5405 - accuracy: 0.7536 - val_loss: 0.5540 - val_accuracy: 0.7968\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5765 - accuracy: 0.7393 - val_loss: 0.4329 - val_accuracy: 0.7914\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5400 - accuracy: 0.7536 - val_loss: 0.4907 - val_accuracy: 0.7968\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6215 - accuracy: 0.7339 - val_loss: 0.7916 - val_accuracy: 0.7219\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7821 - accuracy: 0.7464 - val_loss: 0.7398 - val_accuracy: 0.7647\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7076 - accuracy: 0.7196 - val_loss: 0.4610 - val_accuracy: 0.8128\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6573 - accuracy: 0.7304 - val_loss: 1.0092 - val_accuracy: 0.7968\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7386 - accuracy: 0.7357 - val_loss: 0.4881 - val_accuracy: 0.8182\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6832 - accuracy: 0.7429 - val_loss: 0.8659 - val_accuracy: 0.7112\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1.1179 - accuracy: 0.7321 - val_loss: 0.5205 - val_accuracy: 0.8075\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.0288 - accuracy: 0.6786 - val_loss: 0.9175 - val_accuracy: 0.7968\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.1128 - accuracy: 0.6804 - val_loss: 1.4995 - val_accuracy: 0.7968\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1.3634 - accuracy: 0.6964 - val_loss: 2.0603 - val_accuracy: 0.4920\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.2507 - accuracy: 0.6946 - val_loss: 0.5525 - val_accuracy: 0.8021\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7138 - accuracy: 0.7321 - val_loss: 0.7564 - val_accuracy: 0.7647\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6196 - accuracy: 0.7429 - val_loss: 0.4330 - val_accuracy: 0.8182\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5588 - accuracy: 0.7518 - val_loss: 0.4454 - val_accuracy: 0.8235\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6593 - accuracy: 0.7571 - val_loss: 0.4461 - val_accuracy: 0.8289\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6182 - accuracy: 0.7571 - val_loss: 0.4559 - val_accuracy: 0.8182\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5851 - accuracy: 0.7536 - val_loss: 0.7137 - val_accuracy: 0.7968\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5765 - accuracy: 0.7500 - val_loss: 0.4739 - val_accuracy: 0.7968\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.7268 - val_loss: 0.5873 - val_accuracy: 0.8021\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8868 - accuracy: 0.7125 - val_loss: 0.4324 - val_accuracy: 0.8235\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7104 - accuracy: 0.7018 - val_loss: 0.6421 - val_accuracy: 0.7968\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.7375 - val_loss: 0.4514 - val_accuracy: 0.8182\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6553 - accuracy: 0.7232 - val_loss: 0.4344 - val_accuracy: 0.8182\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7161 - accuracy: 0.7571 - val_loss: 0.4580 - val_accuracy: 0.7968\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8253 - accuracy: 0.7000 - val_loss: 0.7715 - val_accuracy: 0.7968\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.3504 - accuracy: 0.7036 - val_loss: 1.4109 - val_accuracy: 0.6043\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.1053 - accuracy: 0.6607 - val_loss: 0.8579 - val_accuracy: 0.7112\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.4998 - accuracy: 0.6589 - val_loss: 0.7088 - val_accuracy: 0.7968\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.1945 - accuracy: 0.7250 - val_loss: 0.4334 - val_accuracy: 0.8021\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9624 - accuracy: 0.7071 - val_loss: 0.8221 - val_accuracy: 0.7968\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6713 - accuracy: 0.7429 - val_loss: 0.5518 - val_accuracy: 0.7968\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6191 - accuracy: 0.7393 - val_loss: 0.4322 - val_accuracy: 0.7968\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9118 - accuracy: 0.7214 - val_loss: 0.5575 - val_accuracy: 0.8075\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.0663 - accuracy: 0.6821 - val_loss: 0.6834 - val_accuracy: 0.7968\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7447 - accuracy: 0.7125 - val_loss: 0.4529 - val_accuracy: 0.8182\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6871 - accuracy: 0.7286 - val_loss: 0.7176 - val_accuracy: 0.7701\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7452 - accuracy: 0.7107 - val_loss: 0.4307 - val_accuracy: 0.8182\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6313 - accuracy: 0.7464 - val_loss: 0.5318 - val_accuracy: 0.8075\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6195 - accuracy: 0.7446 - val_loss: 0.5398 - val_accuracy: 0.8075\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5790 - accuracy: 0.7339 - val_loss: 0.5748 - val_accuracy: 0.8021\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6042 - accuracy: 0.7446 - val_loss: 0.5231 - val_accuracy: 0.7968\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7982 - accuracy: 0.7161 - val_loss: 0.8640 - val_accuracy: 0.7968\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7689 - accuracy: 0.7071 - val_loss: 0.8278 - val_accuracy: 0.7968\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6773 - accuracy: 0.7411 - val_loss: 1.1877 - val_accuracy: 0.7968\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6471 - accuracy: 0.7429 - val_loss: 0.4324 - val_accuracy: 0.8235\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5676 - accuracy: 0.7482 - val_loss: 0.6013 - val_accuracy: 0.7968\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6326 - accuracy: 0.7321 - val_loss: 0.6615 - val_accuracy: 0.7968\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6309 - accuracy: 0.7375 - val_loss: 0.6712 - val_accuracy: 0.7754\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.1661 - accuracy: 0.6804 - val_loss: 0.4587 - val_accuracy: 0.7968\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.0971 - accuracy: 0.6982 - val_loss: 1.4298 - val_accuracy: 0.7968\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8815 - accuracy: 0.7214 - val_loss: 0.7038 - val_accuracy: 0.7968\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7933 - accuracy: 0.7286 - val_loss: 1.2946 - val_accuracy: 0.7968\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.2099 - accuracy: 0.6929 - val_loss: 0.9448 - val_accuracy: 0.6845\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.7321 - val_loss: 0.4308 - val_accuracy: 0.7914\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5469 - accuracy: 0.7482 - val_loss: 0.4307 - val_accuracy: 0.8128\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6007 - accuracy: 0.7321 - val_loss: 0.7232 - val_accuracy: 0.7968\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7793 - accuracy: 0.7232 - val_loss: 1.2856 - val_accuracy: 0.7968\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8502 - accuracy: 0.7304 - val_loss: 0.9435 - val_accuracy: 0.7968\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8931 - accuracy: 0.7464 - val_loss: 1.0029 - val_accuracy: 0.6845\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7927 - accuracy: 0.7214 - val_loss: 1.1685 - val_accuracy: 0.6684\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8446 - accuracy: 0.7268 - val_loss: 1.0878 - val_accuracy: 0.6845\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9250 - accuracy: 0.7304 - val_loss: 1.0566 - val_accuracy: 0.6845\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.3121 - accuracy: 0.6696 - val_loss: 1.6269 - val_accuracy: 0.7968\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.1733 - accuracy: 0.6982 - val_loss: 1.4061 - val_accuracy: 0.6096\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7265 - accuracy: 0.7304 - val_loss: 0.6603 - val_accuracy: 0.7807\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5723 - accuracy: 0.7482 - val_loss: 0.5284 - val_accuracy: 0.7968\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6216 - accuracy: 0.7607 - val_loss: 0.4699 - val_accuracy: 0.8021\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5610 - accuracy: 0.7411 - val_loss: 0.4353 - val_accuracy: 0.8235\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6331 - accuracy: 0.7429 - val_loss: 0.7054 - val_accuracy: 0.7701\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.7429 - val_loss: 0.5068 - val_accuracy: 0.7968\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6168 - accuracy: 0.7393 - val_loss: 0.9326 - val_accuracy: 0.6845\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8912 - accuracy: 0.7018 - val_loss: 0.4434 - val_accuracy: 0.8289\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6182 - accuracy: 0.7304 - val_loss: 0.6816 - val_accuracy: 0.7754\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6390 - accuracy: 0.7500 - val_loss: 0.6158 - val_accuracy: 0.7968\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6153 - accuracy: 0.7446 - val_loss: 0.6726 - val_accuracy: 0.7754\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7179 - accuracy: 0.7411 - val_loss: 1.1845 - val_accuracy: 0.6684\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.2446 - accuracy: 0.6625 - val_loss: 0.4810 - val_accuracy: 0.7968\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5970 - accuracy: 0.7482 - val_loss: 0.8031 - val_accuracy: 0.7326\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6196 - accuracy: 0.7411 - val_loss: 0.7128 - val_accuracy: 0.7701\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5865 - accuracy: 0.7464 - val_loss: 0.4552 - val_accuracy: 0.7968\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6012 - accuracy: 0.7411 - val_loss: 0.7836 - val_accuracy: 0.7968\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.8855 - accuracy: 0.6929 - val_loss: 1.2472 - val_accuracy: 0.7968\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8274 - accuracy: 0.7286 - val_loss: 0.4449 - val_accuracy: 0.8021\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8163 - accuracy: 0.7018 - val_loss: 0.5751 - val_accuracy: 0.7968\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6746 - accuracy: 0.7518 - val_loss: 0.4319 - val_accuracy: 0.8235\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6050 - accuracy: 0.7321 - val_loss: 1.0013 - val_accuracy: 0.7968\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8450 - accuracy: 0.7089 - val_loss: 0.4327 - val_accuracy: 0.8235\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.7482 - val_loss: 1.1893 - val_accuracy: 0.7968\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6164 - accuracy: 0.7411 - val_loss: 0.4355 - val_accuracy: 0.8289\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5956 - accuracy: 0.7446 - val_loss: 0.8344 - val_accuracy: 0.7968\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9654 - accuracy: 0.7393 - val_loss: 0.7792 - val_accuracy: 0.7433\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.7400 - accuracy: 0.7054 - val_loss: 0.7143 - val_accuracy: 0.7968\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6685 - accuracy: 0.7464 - val_loss: 0.6397 - val_accuracy: 0.7754\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7935 - accuracy: 0.7304 - val_loss: 0.5042 - val_accuracy: 0.7968\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7909 - accuracy: 0.6946 - val_loss: 0.9841 - val_accuracy: 0.7968\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8704 - accuracy: 0.7107 - val_loss: 0.5574 - val_accuracy: 0.7968\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.7375 - val_loss: 0.5507 - val_accuracy: 0.7968\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7042 - accuracy: 0.7161 - val_loss: 0.4565 - val_accuracy: 0.8182\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5845 - accuracy: 0.7357 - val_loss: 0.7211 - val_accuracy: 0.7647\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6773 - accuracy: 0.7107 - val_loss: 0.5276 - val_accuracy: 0.7968\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5876 - accuracy: 0.7446 - val_loss: 0.5068 - val_accuracy: 0.7968\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5548 - accuracy: 0.7536 - val_loss: 0.4862 - val_accuracy: 0.8182\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6176 - accuracy: 0.7339 - val_loss: 0.6721 - val_accuracy: 0.7754\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6578 - accuracy: 0.7304 - val_loss: 0.4538 - val_accuracy: 0.8128\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7554 - val_loss: 0.4658 - val_accuracy: 0.8021\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6250 - accuracy: 0.7304 - val_loss: 0.5952 - val_accuracy: 0.7968\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6052 - accuracy: 0.7357 - val_loss: 0.5164 - val_accuracy: 0.8128\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5389 - accuracy: 0.7554 - val_loss: 0.6980 - val_accuracy: 0.7807\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5780 - accuracy: 0.7393 - val_loss: 0.5392 - val_accuracy: 0.8075\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.8302 - accuracy: 0.7196 - val_loss: 0.7704 - val_accuracy: 0.7433\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6526 - accuracy: 0.7357 - val_loss: 0.4656 - val_accuracy: 0.8182\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.6311 - accuracy: 0.7161 - val_loss: 1.0799 - val_accuracy: 0.7968\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.8786 - accuracy: 0.7214 - val_loss: 0.4751 - val_accuracy: 0.7968\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.0013 - accuracy: 0.7036 - val_loss: 0.5607 - val_accuracy: 0.8021\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6736 - accuracy: 0.7179 - val_loss: 0.4399 - val_accuracy: 0.8075\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5510 - accuracy: 0.7446 - val_loss: 0.4421 - val_accuracy: 0.8235\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5742 - accuracy: 0.7339 - val_loss: 0.4486 - val_accuracy: 0.8021\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6018 - accuracy: 0.7607 - val_loss: 0.6201 - val_accuracy: 0.7861\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6282 - accuracy: 0.7286 - val_loss: 0.7205 - val_accuracy: 0.7968\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6289 - accuracy: 0.7304 - val_loss: 0.4391 - val_accuracy: 0.8075\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6321 - accuracy: 0.7429 - val_loss: 0.4332 - val_accuracy: 0.7968\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5871 - accuracy: 0.7375 - val_loss: 0.7172 - val_accuracy: 0.7647\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7487 - accuracy: 0.7250 - val_loss: 0.5124 - val_accuracy: 0.7968\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6309 - accuracy: 0.7268 - val_loss: 0.8679 - val_accuracy: 0.7968\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.9371 - accuracy: 0.7018 - val_loss: 1.2960 - val_accuracy: 0.7968\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7831 - accuracy: 0.7250 - val_loss: 0.4399 - val_accuracy: 0.8075\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.0125 - accuracy: 0.6911 - val_loss: 0.5129 - val_accuracy: 0.7968\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.8086 - accuracy: 0.7071 - val_loss: 0.9894 - val_accuracy: 0.7968\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.8007 - accuracy: 0.7143 - val_loss: 0.7248 - val_accuracy: 0.7647\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.1254 - accuracy: 0.6786 - val_loss: 1.3260 - val_accuracy: 0.7968\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.8480 - accuracy: 0.7179 - val_loss: 0.6273 - val_accuracy: 0.7968\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5944 - accuracy: 0.7321 - val_loss: 0.9269 - val_accuracy: 0.6791\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7011 - accuracy: 0.7446 - val_loss: 0.4418 - val_accuracy: 0.8289\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.8447 - accuracy: 0.7143 - val_loss: 0.4675 - val_accuracy: 0.8021\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9434 - accuracy: 0.6893 - val_loss: 0.7540 - val_accuracy: 0.7968\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6488 - accuracy: 0.7357 - val_loss: 0.6050 - val_accuracy: 0.7968\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5655 - accuracy: 0.7464 - val_loss: 0.4389 - val_accuracy: 0.8075\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5812 - accuracy: 0.7589 - val_loss: 0.4322 - val_accuracy: 0.8021\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6574 - accuracy: 0.7446 - val_loss: 0.7426 - val_accuracy: 0.7968\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6731 - accuracy: 0.7196 - val_loss: 0.5036 - val_accuracy: 0.7968\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.0885 - accuracy: 0.7143 - val_loss: 0.7183 - val_accuracy: 0.7754\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6671 - accuracy: 0.7429 - val_loss: 0.5681 - val_accuracy: 0.7968\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5396 - accuracy: 0.7625 - val_loss: 0.5012 - val_accuracy: 0.7968\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 0.7268 - val_loss: 0.4663 - val_accuracy: 0.8021\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5442 - accuracy: 0.7464 - val_loss: 0.4843 - val_accuracy: 0.7968\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5850 - accuracy: 0.7482 - val_loss: 0.4712 - val_accuracy: 0.8182\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6161 - accuracy: 0.7393 - val_loss: 0.5094 - val_accuracy: 0.8075\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5557 - accuracy: 0.7375 - val_loss: 0.4336 - val_accuracy: 0.7914\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6614 - accuracy: 0.7393 - val_loss: 0.5424 - val_accuracy: 0.7968\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6775 - accuracy: 0.7375 - val_loss: 0.4631 - val_accuracy: 0.8021\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7358 - accuracy: 0.7304 - val_loss: 1.1413 - val_accuracy: 0.6684\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6944 - accuracy: 0.7089 - val_loss: 0.8561 - val_accuracy: 0.7112\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6755 - accuracy: 0.7268 - val_loss: 0.6222 - val_accuracy: 0.7861\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5723 - accuracy: 0.7339 - val_loss: 0.4946 - val_accuracy: 0.7968\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5527 - accuracy: 0.7500 - val_loss: 0.6434 - val_accuracy: 0.7968\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6465 - accuracy: 0.7357 - val_loss: 0.4856 - val_accuracy: 0.7968\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5734 - accuracy: 0.7500 - val_loss: 0.5931 - val_accuracy: 0.7968\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6445 - accuracy: 0.7232 - val_loss: 0.7921 - val_accuracy: 0.7968\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6371 - accuracy: 0.7411 - val_loss: 0.4894 - val_accuracy: 0.8128\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6482 - accuracy: 0.7393 - val_loss: 0.5345 - val_accuracy: 0.7968\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5697 - accuracy: 0.7643 - val_loss: 0.5266 - val_accuracy: 0.7968\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7522 - accuracy: 0.7196 - val_loss: 0.4326 - val_accuracy: 0.7914\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7802 - accuracy: 0.7179 - val_loss: 1.0367 - val_accuracy: 0.7968\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.1919 - accuracy: 0.7071 - val_loss: 0.8287 - val_accuracy: 0.7219\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8012 - accuracy: 0.7054 - val_loss: 0.4766 - val_accuracy: 0.8182\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5734 - accuracy: 0.7536 - val_loss: 0.4953 - val_accuracy: 0.7968\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5630 - accuracy: 0.7518 - val_loss: 0.6441 - val_accuracy: 0.7968\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8376 - accuracy: 0.6964 - val_loss: 0.6678 - val_accuracy: 0.7968\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7394 - accuracy: 0.7286 - val_loss: 0.8283 - val_accuracy: 0.7968\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7928 - accuracy: 0.7196 - val_loss: 0.6982 - val_accuracy: 0.7807\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5699 - accuracy: 0.7482 - val_loss: 0.7525 - val_accuracy: 0.7968\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5960 - accuracy: 0.7304 - val_loss: 0.4431 - val_accuracy: 0.8021\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5496 - accuracy: 0.7375 - val_loss: 0.4658 - val_accuracy: 0.8182\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5720 - accuracy: 0.7357 - val_loss: 0.4864 - val_accuracy: 0.8128\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7549 - accuracy: 0.7375 - val_loss: 0.6051 - val_accuracy: 0.7807\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6178 - accuracy: 0.7464 - val_loss: 0.4892 - val_accuracy: 0.7968\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5968 - accuracy: 0.7411 - val_loss: 0.5967 - val_accuracy: 0.7861\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6338 - accuracy: 0.7232 - val_loss: 0.4658 - val_accuracy: 0.8182\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5561 - accuracy: 0.7393 - val_loss: 0.4371 - val_accuracy: 0.8182\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6115 - accuracy: 0.7482 - val_loss: 0.6093 - val_accuracy: 0.7968\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8621 - accuracy: 0.7089 - val_loss: 1.1546 - val_accuracy: 0.7968\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7622 - accuracy: 0.7500 - val_loss: 0.4545 - val_accuracy: 0.7968\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6546 - accuracy: 0.7464 - val_loss: 0.6890 - val_accuracy: 0.7968\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5581 - accuracy: 0.7500 - val_loss: 0.4354 - val_accuracy: 0.8182\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5433 - accuracy: 0.7536 - val_loss: 0.4593 - val_accuracy: 0.8075\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5632 - accuracy: 0.7464 - val_loss: 0.5367 - val_accuracy: 0.7968\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6518 - accuracy: 0.7393 - val_loss: 0.4376 - val_accuracy: 0.8075\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5495 - accuracy: 0.7464 - val_loss: 0.5492 - val_accuracy: 0.8075\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5656 - accuracy: 0.7536 - val_loss: 0.4776 - val_accuracy: 0.7968\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5615 - accuracy: 0.7429 - val_loss: 0.9653 - val_accuracy: 0.6845\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6609 - accuracy: 0.7250 - val_loss: 0.5961 - val_accuracy: 0.7861\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6398 - accuracy: 0.7286 - val_loss: 0.5155 - val_accuracy: 0.7968\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6309 - accuracy: 0.7375 - val_loss: 1.1364 - val_accuracy: 0.7968\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7232 - accuracy: 0.7232 - val_loss: 0.9924 - val_accuracy: 0.7968\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7376 - accuracy: 0.7286 - val_loss: 0.6240 - val_accuracy: 0.7754\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6188 - accuracy: 0.7375 - val_loss: 0.4841 - val_accuracy: 0.7968\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5424 - accuracy: 0.7393 - val_loss: 0.5548 - val_accuracy: 0.8021\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7008 - accuracy: 0.7179 - val_loss: 0.5064 - val_accuracy: 0.8075\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5604 - accuracy: 0.7482 - val_loss: 0.8634 - val_accuracy: 0.7968\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.9304 - accuracy: 0.7089 - val_loss: 0.5754 - val_accuracy: 0.7968\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6668 - accuracy: 0.7482 - val_loss: 0.4330 - val_accuracy: 0.7914\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5722 - accuracy: 0.7464 - val_loss: 0.8370 - val_accuracy: 0.7968\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8938 - accuracy: 0.7196 - val_loss: 0.6390 - val_accuracy: 0.7968\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6113 - accuracy: 0.7321 - val_loss: 0.5177 - val_accuracy: 0.7968\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6560 - accuracy: 0.7286 - val_loss: 0.7885 - val_accuracy: 0.7968\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.9694 - accuracy: 0.6875 - val_loss: 0.5949 - val_accuracy: 0.7968\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7235 - accuracy: 0.7393 - val_loss: 0.7608 - val_accuracy: 0.7968\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6368 - accuracy: 0.7429 - val_loss: 0.4405 - val_accuracy: 0.8075\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5575 - accuracy: 0.7571 - val_loss: 0.6696 - val_accuracy: 0.7754\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7069 - accuracy: 0.7268 - val_loss: 0.9516 - val_accuracy: 0.6845\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7277 - accuracy: 0.7339 - val_loss: 0.4537 - val_accuracy: 0.8075\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5553 - accuracy: 0.7482 - val_loss: 0.5120 - val_accuracy: 0.8075\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5634 - accuracy: 0.7375 - val_loss: 0.4455 - val_accuracy: 0.8021\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5244 - accuracy: 0.7500 - val_loss: 0.4863 - val_accuracy: 0.8182\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7541 - accuracy: 0.6911 - val_loss: 0.9970 - val_accuracy: 0.7968\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.8320 - accuracy: 0.7214 - val_loss: 0.5152 - val_accuracy: 0.7968\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8081 - accuracy: 0.6982 - val_loss: 0.4332 - val_accuracy: 0.8128\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5623 - accuracy: 0.7571 - val_loss: 0.4587 - val_accuracy: 0.8021\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5446 - accuracy: 0.7464 - val_loss: 0.4428 - val_accuracy: 0.8289\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6198 - accuracy: 0.7446 - val_loss: 0.5031 - val_accuracy: 0.8128\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6810 - accuracy: 0.7214 - val_loss: 0.5010 - val_accuracy: 0.7968\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6284 - accuracy: 0.7554 - val_loss: 0.8028 - val_accuracy: 0.7968\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6367 - accuracy: 0.7321 - val_loss: 0.8095 - val_accuracy: 0.7968\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6589 - accuracy: 0.7375 - val_loss: 0.5543 - val_accuracy: 0.8021\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7215 - accuracy: 0.7232 - val_loss: 0.6721 - val_accuracy: 0.7754\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6758 - accuracy: 0.7321 - val_loss: 0.5091 - val_accuracy: 0.7968\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5960 - accuracy: 0.7286 - val_loss: 0.5892 - val_accuracy: 0.7968\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6806 - accuracy: 0.7357 - val_loss: 0.6988 - val_accuracy: 0.7968\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6660 - accuracy: 0.7268 - val_loss: 0.4385 - val_accuracy: 0.8075\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6028 - accuracy: 0.7375 - val_loss: 0.4974 - val_accuracy: 0.8182\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6801 - accuracy: 0.7143 - val_loss: 0.7220 - val_accuracy: 0.7968\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.4808 - accuracy: 0.6839 - val_loss: 0.8717 - val_accuracy: 0.7968\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6326 - accuracy: 0.7446 - val_loss: 0.7426 - val_accuracy: 0.7968\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6288 - accuracy: 0.7232 - val_loss: 0.7690 - val_accuracy: 0.7968\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7236 - accuracy: 0.7179 - val_loss: 0.8683 - val_accuracy: 0.7968\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6155 - accuracy: 0.7429 - val_loss: 0.4897 - val_accuracy: 0.7968\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5751 - accuracy: 0.7482 - val_loss: 0.4331 - val_accuracy: 0.8021\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6204 - accuracy: 0.7196 - val_loss: 0.4909 - val_accuracy: 0.7968\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6865 - accuracy: 0.7375 - val_loss: 0.4517 - val_accuracy: 0.7968\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5755 - accuracy: 0.7321 - val_loss: 0.4796 - val_accuracy: 0.7968\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6562 - accuracy: 0.7286 - val_loss: 0.6311 - val_accuracy: 0.7807\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7021 - accuracy: 0.7339 - val_loss: 0.5302 - val_accuracy: 0.8182\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6473 - accuracy: 0.7446 - val_loss: 0.6588 - val_accuracy: 0.7754\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7423 - accuracy: 0.7268 - val_loss: 0.4716 - val_accuracy: 0.8182\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7762 - accuracy: 0.7268 - val_loss: 0.6880 - val_accuracy: 0.7807\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6799 - accuracy: 0.7357 - val_loss: 0.4328 - val_accuracy: 0.8021\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5795 - accuracy: 0.7357 - val_loss: 0.4598 - val_accuracy: 0.8075\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6451 - accuracy: 0.7375 - val_loss: 0.6481 - val_accuracy: 0.7807\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7931 - accuracy: 0.7179 - val_loss: 0.4386 - val_accuracy: 0.8075\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5602 - accuracy: 0.7589 - val_loss: 0.4344 - val_accuracy: 0.8182\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5770 - accuracy: 0.7411 - val_loss: 0.4323 - val_accuracy: 0.8128\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5831 - accuracy: 0.7464 - val_loss: 0.4375 - val_accuracy: 0.8075\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6427 - accuracy: 0.7286 - val_loss: 0.5085 - val_accuracy: 0.8075\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5989 - accuracy: 0.7393 - val_loss: 0.4971 - val_accuracy: 0.8182\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6327 - accuracy: 0.7393 - val_loss: 0.5535 - val_accuracy: 0.7968\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5877 - accuracy: 0.7411 - val_loss: 0.5225 - val_accuracy: 0.7968\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.9964 - accuracy: 0.6946 - val_loss: 1.4690 - val_accuracy: 0.5722\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.7992 - accuracy: 0.7179 - val_loss: 0.5512 - val_accuracy: 0.8075\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6482 - accuracy: 0.7411 - val_loss: 0.7331 - val_accuracy: 0.7754\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.7861 - accuracy: 0.7071 - val_loss: 0.4784 - val_accuracy: 0.7968\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.8378 - accuracy: 0.6929 - val_loss: 1.5207 - val_accuracy: 0.7968\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 1.1349 - accuracy: 0.7125 - val_loss: 1.0856 - val_accuracy: 0.6738\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.8174 - accuracy: 0.7107 - val_loss: 0.8579 - val_accuracy: 0.7968\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6809 - accuracy: 0.7482 - val_loss: 0.5410 - val_accuracy: 0.7968\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6084 - accuracy: 0.7411 - val_loss: 0.4521 - val_accuracy: 0.7968\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5871 - accuracy: 0.7625 - val_loss: 0.5043 - val_accuracy: 0.8128\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5844 - accuracy: 0.7411 - val_loss: 0.4321 - val_accuracy: 0.8182\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6301 - accuracy: 0.7446 - val_loss: 0.6194 - val_accuracy: 0.7807\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7052 - accuracy: 0.7161 - val_loss: 0.4625 - val_accuracy: 0.8021\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6107 - accuracy: 0.7286 - val_loss: 0.5459 - val_accuracy: 0.8075\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5592 - accuracy: 0.7482 - val_loss: 0.4712 - val_accuracy: 0.8182\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7429 - val_loss: 0.4311 - val_accuracy: 0.8021\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5552 - accuracy: 0.7625 - val_loss: 0.4554 - val_accuracy: 0.8128\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6889 - accuracy: 0.7125 - val_loss: 0.4987 - val_accuracy: 0.7968\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6395 - accuracy: 0.7518 - val_loss: 0.4346 - val_accuracy: 0.8021\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5511 - accuracy: 0.7357 - val_loss: 0.7815 - val_accuracy: 0.7968\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7065 - accuracy: 0.7125 - val_loss: 0.4314 - val_accuracy: 0.7914\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5419 - accuracy: 0.7589 - val_loss: 0.5822 - val_accuracy: 0.7861\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5966 - accuracy: 0.7232 - val_loss: 0.4459 - val_accuracy: 0.8021\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7392 - accuracy: 0.7125 - val_loss: 0.4554 - val_accuracy: 0.7968\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.9758 - accuracy: 0.7232 - val_loss: 0.4569 - val_accuracy: 0.8075\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5871 - accuracy: 0.7411 - val_loss: 0.4549 - val_accuracy: 0.7968\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6333 - accuracy: 0.7214 - val_loss: 0.5783 - val_accuracy: 0.7968\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7111 - accuracy: 0.7196 - val_loss: 0.7115 - val_accuracy: 0.7968\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6522 - accuracy: 0.7429 - val_loss: 0.4701 - val_accuracy: 0.7968\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8148 - accuracy: 0.7286 - val_loss: 0.4489 - val_accuracy: 0.7968\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7512 - accuracy: 0.7393 - val_loss: 0.7862 - val_accuracy: 0.7380\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5955 - accuracy: 0.7411 - val_loss: 0.5237 - val_accuracy: 0.8182\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5655 - accuracy: 0.7500 - val_loss: 0.5094 - val_accuracy: 0.8075\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5545 - accuracy: 0.7571 - val_loss: 0.7122 - val_accuracy: 0.7968\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7089 - accuracy: 0.7161 - val_loss: 0.5339 - val_accuracy: 0.7968\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5668 - accuracy: 0.7446 - val_loss: 0.4333 - val_accuracy: 0.8182\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5442 - accuracy: 0.7536 - val_loss: 0.4308 - val_accuracy: 0.7914\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5538 - accuracy: 0.7536 - val_loss: 0.5195 - val_accuracy: 0.8128\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5540 - accuracy: 0.7554 - val_loss: 0.4453 - val_accuracy: 0.8128\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6672 - accuracy: 0.7304 - val_loss: 0.5699 - val_accuracy: 0.7968\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6349 - accuracy: 0.7339 - val_loss: 0.4328 - val_accuracy: 0.8021\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5939 - accuracy: 0.7446 - val_loss: 1.2228 - val_accuracy: 0.6364\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7209 - accuracy: 0.7250 - val_loss: 0.4494 - val_accuracy: 0.8182\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7500 - val_loss: 0.5310 - val_accuracy: 0.7968\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5540 - accuracy: 0.7500 - val_loss: 0.4468 - val_accuracy: 0.8021\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5651 - accuracy: 0.7554 - val_loss: 0.4417 - val_accuracy: 0.8235\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5344 - accuracy: 0.7518 - val_loss: 0.5603 - val_accuracy: 0.8021\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7771 - accuracy: 0.7357 - val_loss: 0.5164 - val_accuracy: 0.7968\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6737 - accuracy: 0.7161 - val_loss: 0.5108 - val_accuracy: 0.7968\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7295 - accuracy: 0.7179 - val_loss: 0.5000 - val_accuracy: 0.8182\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6380 - accuracy: 0.7232 - val_loss: 0.8969 - val_accuracy: 0.7968\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5866 - accuracy: 0.7482 - val_loss: 0.4355 - val_accuracy: 0.8235\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5383 - accuracy: 0.7429 - val_loss: 0.4670 - val_accuracy: 0.8128\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5888 - accuracy: 0.7482 - val_loss: 0.4757 - val_accuracy: 0.7968\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6275 - accuracy: 0.7429 - val_loss: 0.4355 - val_accuracy: 0.8075\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5429 - accuracy: 0.7518 - val_loss: 0.4387 - val_accuracy: 0.8075\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5648 - accuracy: 0.7429 - val_loss: 0.4365 - val_accuracy: 0.8182\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5684 - accuracy: 0.7464 - val_loss: 0.5282 - val_accuracy: 0.7968\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6646 - accuracy: 0.7304 - val_loss: 0.7445 - val_accuracy: 0.7968\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5941 - accuracy: 0.7446 - val_loss: 0.4345 - val_accuracy: 0.8021\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5470 - accuracy: 0.7464 - val_loss: 0.4863 - val_accuracy: 0.7968\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6224 - accuracy: 0.7464 - val_loss: 0.5848 - val_accuracy: 0.7968\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.8141 - accuracy: 0.7018 - val_loss: 0.8558 - val_accuracy: 0.7968\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6495 - accuracy: 0.7357 - val_loss: 0.5048 - val_accuracy: 0.8128\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6678 - accuracy: 0.7250 - val_loss: 1.2486 - val_accuracy: 0.6257\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8060 - accuracy: 0.7250 - val_loss: 0.4341 - val_accuracy: 0.8021\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6145 - accuracy: 0.7232 - val_loss: 0.4603 - val_accuracy: 0.8075\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5583 - accuracy: 0.7464 - val_loss: 0.4703 - val_accuracy: 0.7968\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5524 - accuracy: 0.7518 - val_loss: 0.4332 - val_accuracy: 0.7914\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5364 - accuracy: 0.7446 - val_loss: 0.4373 - val_accuracy: 0.8075\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6187 - accuracy: 0.7375 - val_loss: 0.4491 - val_accuracy: 0.8182\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5665 - accuracy: 0.7375 - val_loss: 0.4860 - val_accuracy: 0.7968\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5708 - accuracy: 0.7393 - val_loss: 0.6681 - val_accuracy: 0.7968\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6089 - accuracy: 0.7232 - val_loss: 0.4631 - val_accuracy: 0.8021\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5556 - accuracy: 0.7536 - val_loss: 0.7287 - val_accuracy: 0.7968\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6154 - accuracy: 0.7411 - val_loss: 0.6302 - val_accuracy: 0.7807\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5698 - accuracy: 0.7464 - val_loss: 0.4352 - val_accuracy: 0.8021\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6647 - accuracy: 0.7304 - val_loss: 0.4847 - val_accuracy: 0.8182\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7195 - accuracy: 0.7214 - val_loss: 0.7330 - val_accuracy: 0.7968\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6035 - accuracy: 0.7375 - val_loss: 0.5599 - val_accuracy: 0.8021\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6308 - accuracy: 0.7518 - val_loss: 0.5389 - val_accuracy: 0.8128\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7537 - accuracy: 0.7089 - val_loss: 0.5984 - val_accuracy: 0.7807\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.7362 - accuracy: 0.7071 - val_loss: 0.6741 - val_accuracy: 0.7968\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5566 - accuracy: 0.7429 - val_loss: 0.5282 - val_accuracy: 0.7968\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5903 - accuracy: 0.7429 - val_loss: 0.4360 - val_accuracy: 0.8021\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5474 - accuracy: 0.7375 - val_loss: 0.4794 - val_accuracy: 0.7968\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6460 - accuracy: 0.7196 - val_loss: 0.4473 - val_accuracy: 0.7968\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5359 - accuracy: 0.7536 - val_loss: 0.5658 - val_accuracy: 0.7968\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5574 - accuracy: 0.7643 - val_loss: 0.6625 - val_accuracy: 0.7754\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5953 - accuracy: 0.7268 - val_loss: 0.5205 - val_accuracy: 0.8128\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5777 - accuracy: 0.7536 - val_loss: 0.5997 - val_accuracy: 0.7807\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5827 - accuracy: 0.7518 - val_loss: 0.4801 - val_accuracy: 0.8182\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5961 - accuracy: 0.7357 - val_loss: 0.4386 - val_accuracy: 0.8182\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5378 - accuracy: 0.7589 - val_loss: 0.4875 - val_accuracy: 0.7968\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.9953 - accuracy: 0.6732 - val_loss: 1.3683 - val_accuracy: 0.7968\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.9559 - accuracy: 0.7125 - val_loss: 0.4721 - val_accuracy: 0.8128\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7099 - accuracy: 0.7143 - val_loss: 0.4682 - val_accuracy: 0.7968\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5966 - accuracy: 0.7518 - val_loss: 0.4599 - val_accuracy: 0.8075\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5581 - accuracy: 0.7268 - val_loss: 0.6329 - val_accuracy: 0.7968\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5898 - accuracy: 0.7482 - val_loss: 0.6533 - val_accuracy: 0.7968\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.6529 - accuracy: 0.7357 - val_loss: 0.6965 - val_accuracy: 0.7968\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6446 - accuracy: 0.7250 - val_loss: 0.4941 - val_accuracy: 0.7968\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6015 - accuracy: 0.7339 - val_loss: 0.4990 - val_accuracy: 0.7968\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.5485 - accuracy: 0.7446 - val_loss: 0.4583 - val_accuracy: 0.8021\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5786 - accuracy: 0.7375 - val_loss: 0.4566 - val_accuracy: 0.8021\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.5769 - accuracy: 0.7429 - val_loss: 0.5015 - val_accuracy: 0.8128\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5522 - accuracy: 0.7482 - val_loss: 0.4512 - val_accuracy: 0.8075\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5241 - accuracy: 0.7679 - val_loss: 0.6651 - val_accuracy: 0.7754\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6188 - accuracy: 0.7286 - val_loss: 0.5548 - val_accuracy: 0.8075\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5608 - accuracy: 0.7446 - val_loss: 0.7484 - val_accuracy: 0.7594\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5970 - accuracy: 0.7250 - val_loss: 0.4470 - val_accuracy: 0.8182\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.5732 - accuracy: 0.7411 - val_loss: 0.4514 - val_accuracy: 0.8128\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5617 - accuracy: 0.7518 - val_loss: 0.6796 - val_accuracy: 0.7968\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.7203 - accuracy: 0.7232 - val_loss: 0.7630 - val_accuracy: 0.7968\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6552 - accuracy: 0.7179 - val_loss: 0.4345 - val_accuracy: 0.8021\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5589 - accuracy: 0.7571 - val_loss: 0.4432 - val_accuracy: 0.8021\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5934 - accuracy: 0.7339 - val_loss: 0.5668 - val_accuracy: 0.7968\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.6039 - accuracy: 0.7214 - val_loss: 0.5053 - val_accuracy: 0.7968\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5696 - accuracy: 0.7393 - val_loss: 0.4423 - val_accuracy: 0.8289\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6102 - accuracy: 0.7250 - val_loss: 0.4342 - val_accuracy: 0.8021\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5388 - accuracy: 0.7464 - val_loss: 0.5176 - val_accuracy: 0.8075\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.7823 - accuracy: 0.7161 - val_loss: 0.5410 - val_accuracy: 0.8128\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.8928 - accuracy: 0.7018 - val_loss: 1.1865 - val_accuracy: 0.7968\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.9279 - accuracy: 0.7196 - val_loss: 0.7289 - val_accuracy: 0.7807\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6286 - accuracy: 0.7429 - val_loss: 0.4389 - val_accuracy: 0.8128\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5794 - accuracy: 0.7375 - val_loss: 0.5088 - val_accuracy: 0.8075\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6198 - accuracy: 0.7268 - val_loss: 0.6135 - val_accuracy: 0.7754\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5515 - accuracy: 0.7446 - val_loss: 0.5402 - val_accuracy: 0.8128\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5882 - accuracy: 0.7411 - val_loss: 0.4350 - val_accuracy: 0.8182\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5331 - accuracy: 0.7500 - val_loss: 0.4484 - val_accuracy: 0.8182\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5912 - accuracy: 0.7411 - val_loss: 0.6411 - val_accuracy: 0.7807\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5499 - accuracy: 0.7446 - val_loss: 0.4707 - val_accuracy: 0.8128\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5917 - accuracy: 0.7321 - val_loss: 0.4728 - val_accuracy: 0.7968\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5725 - accuracy: 0.7375 - val_loss: 0.4369 - val_accuracy: 0.8075\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6205 - accuracy: 0.7214 - val_loss: 0.6426 - val_accuracy: 0.7968\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7977 - accuracy: 0.7196 - val_loss: 0.8998 - val_accuracy: 0.7968\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6495 - accuracy: 0.7339 - val_loss: 0.4350 - val_accuracy: 0.8021\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5511 - accuracy: 0.7482 - val_loss: 0.4338 - val_accuracy: 0.8021\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5655 - accuracy: 0.7411 - val_loss: 0.4437 - val_accuracy: 0.8021\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5524 - accuracy: 0.7446 - val_loss: 0.4339 - val_accuracy: 0.7968\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5397 - accuracy: 0.7554 - val_loss: 0.4748 - val_accuracy: 0.7968\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6874 - accuracy: 0.7089 - val_loss: 0.7029 - val_accuracy: 0.7968\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7442 - accuracy: 0.7232 - val_loss: 0.9825 - val_accuracy: 0.6791\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7201 - accuracy: 0.7393 - val_loss: 0.4730 - val_accuracy: 0.7968\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6397 - accuracy: 0.7286 - val_loss: 0.4422 - val_accuracy: 0.8289\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5989 - accuracy: 0.7429 - val_loss: 0.5108 - val_accuracy: 0.7968\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6141 - accuracy: 0.7429 - val_loss: 0.4464 - val_accuracy: 0.8021\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5833 - accuracy: 0.7393 - val_loss: 0.4668 - val_accuracy: 0.8128\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.8128\n",
      "Testing Accuracy: 0.8128342032432556\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       149\n",
      "           1       0.55      0.47      0.51        38\n",
      "\n",
      "    accuracy                           0.81       187\n",
      "   macro avg       0.71      0.69      0.70       187\n",
      "weighted avg       0.80      0.81      0.81       187\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f0094b463b43>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fpr' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load and preprocess your data (replace with your data loading code)\n",
    "data = pd.read_csv('/content/drive/MyDrive/AI ML/blood-tranfer.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = np.array(data)[:, 1:-1]\n",
    "Y = np.array(data)[:, -1]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(11, activation='relu'),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(Y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Testing Accuracy: {}\".format(accuracy[1]))\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_result = model.predict(X_test)\n",
    "Y_result = np.argmax(Y_result, axis=1)\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "classification_rep = classification_report(Y_test, Y_result)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_roc_curve(fpr,tpr):\n",
    "  plt.plot(fpr,tpr)\n",
    "  plt.axis([0,1,0,1])\n",
    "  plt.xlabel('False Positive')\n",
    "  plt.xlabel('True Positive')\n",
    "  plt.show()\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Isnbo0jzRHu4",
    "outputId": "872e1544-5a94-4478-f58a-1cb472cd9129"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 3s 37ms/step - loss: 17.2250 - accuracy: 0.7732 - val_loss: 9.3555 - val_accuracy: 0.7326\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 5.9988 - accuracy: 0.7732 - val_loss: 2.6148 - val_accuracy: 0.7326\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 1.5354 - accuracy: 0.7714 - val_loss: 0.6119 - val_accuracy: 0.6791\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.6569 - accuracy: 0.4804 - val_loss: 0.6939 - val_accuracy: 0.3529\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6860 - accuracy: 0.3411 - val_loss: 0.6793 - val_accuracy: 0.3904\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.6561 - accuracy: 0.4732 - val_loss: 0.6350 - val_accuracy: 0.5668\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5636 - accuracy: 0.7625 - val_loss: 0.5881 - val_accuracy: 0.7326\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.5400 - accuracy: 0.7732 - val_loss: 0.5892 - val_accuracy: 0.7326\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.5369 - accuracy: 0.7732 - val_loss: 0.5832 - val_accuracy: 0.7326\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5309 - accuracy: 0.7732 - val_loss: 0.5814 - val_accuracy: 0.7326\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.5224 - accuracy: 0.7732 - val_loss: 0.5770 - val_accuracy: 0.7326\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5348 - accuracy: 0.7732 - val_loss: 0.5711 - val_accuracy: 0.7326\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.5210 - accuracy: 0.7732 - val_loss: 0.5792 - val_accuracy: 0.7326\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 0.5141 - accuracy: 0.7732 - val_loss: 0.5741 - val_accuracy: 0.7326\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5119 - accuracy: 0.7732 - val_loss: 0.5981 - val_accuracy: 0.7326\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5190 - accuracy: 0.7732 - val_loss: 0.5627 - val_accuracy: 0.7326\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5138 - accuracy: 0.7732 - val_loss: 0.5618 - val_accuracy: 0.7326\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5087 - accuracy: 0.7732 - val_loss: 0.5617 - val_accuracy: 0.7326\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5064 - accuracy: 0.7732 - val_loss: 0.5603 - val_accuracy: 0.7326\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 0.5063 - accuracy: 0.7732 - val_loss: 0.5606 - val_accuracy: 0.7326\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.5084 - accuracy: 0.7732 - val_loss: 0.5600 - val_accuracy: 0.7326\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.5091 - accuracy: 0.7732 - val_loss: 0.5599 - val_accuracy: 0.7326\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.5062 - accuracy: 0.7732 - val_loss: 0.5579 - val_accuracy: 0.7326\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.5038 - accuracy: 0.7732 - val_loss: 0.5577 - val_accuracy: 0.7326\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.7732 - val_loss: 0.5584 - val_accuracy: 0.7326\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5013 - accuracy: 0.7732 - val_loss: 0.5605 - val_accuracy: 0.7326\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4995 - accuracy: 0.7732 - val_loss: 0.5575 - val_accuracy: 0.7326\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5010 - accuracy: 0.7732 - val_loss: 0.5572 - val_accuracy: 0.7326\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5017 - accuracy: 0.7732 - val_loss: 0.5568 - val_accuracy: 0.7326\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.5002 - accuracy: 0.7732 - val_loss: 0.5569 - val_accuracy: 0.7326\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4975 - accuracy: 0.7732 - val_loss: 0.5571 - val_accuracy: 0.7326\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4963 - accuracy: 0.7732 - val_loss: 0.5566 - val_accuracy: 0.7326\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4986 - accuracy: 0.7732 - val_loss: 0.5575 - val_accuracy: 0.7326\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5029 - accuracy: 0.7732 - val_loss: 0.5573 - val_accuracy: 0.7326\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.4947 - accuracy: 0.7732 - val_loss: 0.5562 - val_accuracy: 0.7326\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4955 - accuracy: 0.7732 - val_loss: 0.5651 - val_accuracy: 0.7326\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4967 - accuracy: 0.7732 - val_loss: 0.5544 - val_accuracy: 0.7326\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5041 - accuracy: 0.7732 - val_loss: 0.5572 - val_accuracy: 0.7326\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4956 - accuracy: 0.7732 - val_loss: 0.5575 - val_accuracy: 0.7326\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4931 - accuracy: 0.7732 - val_loss: 0.5596 - val_accuracy: 0.7326\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4929 - accuracy: 0.7732 - val_loss: 0.5661 - val_accuracy: 0.7326\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5070 - accuracy: 0.7732 - val_loss: 0.5536 - val_accuracy: 0.7326\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4977 - accuracy: 0.7732 - val_loss: 0.5803 - val_accuracy: 0.7326\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4978 - accuracy: 0.7732 - val_loss: 0.5535 - val_accuracy: 0.7326\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4989 - accuracy: 0.7732 - val_loss: 0.5539 - val_accuracy: 0.7326\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7732 - val_loss: 0.5571 - val_accuracy: 0.7326\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4948 - accuracy: 0.7732 - val_loss: 0.5601 - val_accuracy: 0.7326\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4912 - accuracy: 0.7732 - val_loss: 0.5634 - val_accuracy: 0.7326\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4936 - accuracy: 0.7732 - val_loss: 0.5535 - val_accuracy: 0.7326\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5131 - accuracy: 0.7732 - val_loss: 0.5535 - val_accuracy: 0.7326\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5045 - accuracy: 0.7732 - val_loss: 0.5637 - val_accuracy: 0.7326\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4939 - accuracy: 0.7732 - val_loss: 0.5525 - val_accuracy: 0.7326\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4932 - accuracy: 0.7732 - val_loss: 0.5644 - val_accuracy: 0.7326\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4916 - accuracy: 0.7732 - val_loss: 0.5586 - val_accuracy: 0.7326\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4909 - accuracy: 0.7732 - val_loss: 0.5564 - val_accuracy: 0.7326\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4924 - accuracy: 0.7732 - val_loss: 0.5547 - val_accuracy: 0.7326\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4957 - accuracy: 0.7732 - val_loss: 0.5634 - val_accuracy: 0.7326\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4901 - accuracy: 0.7732 - val_loss: 0.5532 - val_accuracy: 0.7326\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4937 - accuracy: 0.7732 - val_loss: 0.5599 - val_accuracy: 0.7326\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4929 - accuracy: 0.7732 - val_loss: 0.5532 - val_accuracy: 0.7326\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4895 - accuracy: 0.7732 - val_loss: 0.5638 - val_accuracy: 0.7326\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4963 - accuracy: 0.7732 - val_loss: 0.5528 - val_accuracy: 0.7326\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4937 - accuracy: 0.7732 - val_loss: 0.5517 - val_accuracy: 0.7326\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5082 - accuracy: 0.7732 - val_loss: 0.5520 - val_accuracy: 0.7326\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4892 - accuracy: 0.7732 - val_loss: 0.5653 - val_accuracy: 0.7326\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4882 - accuracy: 0.7732 - val_loss: 0.5525 - val_accuracy: 0.7326\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4915 - accuracy: 0.7732 - val_loss: 0.5617 - val_accuracy: 0.7326\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4905 - accuracy: 0.7732 - val_loss: 0.5528 - val_accuracy: 0.7326\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4917 - accuracy: 0.7732 - val_loss: 0.5604 - val_accuracy: 0.7326\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4910 - accuracy: 0.7732 - val_loss: 0.5699 - val_accuracy: 0.7326\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.7732 - val_loss: 0.5523 - val_accuracy: 0.7326\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4908 - accuracy: 0.7732 - val_loss: 0.5678 - val_accuracy: 0.7326\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4930 - accuracy: 0.7732 - val_loss: 0.5610 - val_accuracy: 0.7326\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4915 - accuracy: 0.7732 - val_loss: 0.5527 - val_accuracy: 0.7326\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4905 - accuracy: 0.7732 - val_loss: 0.5650 - val_accuracy: 0.7326\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4905 - accuracy: 0.7732 - val_loss: 0.5574 - val_accuracy: 0.7326\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4914 - accuracy: 0.7732 - val_loss: 0.5526 - val_accuracy: 0.7326\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4933 - accuracy: 0.7732 - val_loss: 0.5562 - val_accuracy: 0.7326\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4894 - accuracy: 0.7732 - val_loss: 0.5679 - val_accuracy: 0.7326\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.7732 - val_loss: 0.5552 - val_accuracy: 0.7326\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4956 - accuracy: 0.7732 - val_loss: 0.5512 - val_accuracy: 0.7326\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4916 - accuracy: 0.7732 - val_loss: 0.5700 - val_accuracy: 0.7326\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4911 - accuracy: 0.7732 - val_loss: 0.5604 - val_accuracy: 0.7326\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4935 - accuracy: 0.7732 - val_loss: 0.5523 - val_accuracy: 0.7326\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4915 - accuracy: 0.7732 - val_loss: 0.5656 - val_accuracy: 0.7326\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4938 - accuracy: 0.7732 - val_loss: 0.5680 - val_accuracy: 0.7326\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4971 - accuracy: 0.7732 - val_loss: 0.5540 - val_accuracy: 0.7326\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5179 - accuracy: 0.7732 - val_loss: 0.5509 - val_accuracy: 0.7326\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4983 - accuracy: 0.7732 - val_loss: 0.5532 - val_accuracy: 0.7326\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4919 - accuracy: 0.7732 - val_loss: 0.5824 - val_accuracy: 0.7326\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5134 - accuracy: 0.7732 - val_loss: 0.5521 - val_accuracy: 0.7326\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4966 - accuracy: 0.7732 - val_loss: 0.5727 - val_accuracy: 0.7326\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4930 - accuracy: 0.7732 - val_loss: 0.5738 - val_accuracy: 0.7326\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4983 - accuracy: 0.7732 - val_loss: 0.5538 - val_accuracy: 0.7326\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4877 - accuracy: 0.7732 - val_loss: 0.5634 - val_accuracy: 0.7326\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4896 - accuracy: 0.7732 - val_loss: 0.5682 - val_accuracy: 0.7326\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5072 - accuracy: 0.7732 - val_loss: 0.5500 - val_accuracy: 0.7326\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4991 - accuracy: 0.7732 - val_loss: 0.5703 - val_accuracy: 0.7326\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4922 - accuracy: 0.7732 - val_loss: 0.5529 - val_accuracy: 0.7326\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.7732 - val_loss: 0.5531 - val_accuracy: 0.7326\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4932 - accuracy: 0.7732 - val_loss: 0.5983 - val_accuracy: 0.7326\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4895 - accuracy: 0.7732 - val_loss: 0.5553 - val_accuracy: 0.7326\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4862 - accuracy: 0.7732 - val_loss: 0.5641 - val_accuracy: 0.7326\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4866 - accuracy: 0.7732 - val_loss: 0.5768 - val_accuracy: 0.7326\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4976 - accuracy: 0.7732 - val_loss: 0.5526 - val_accuracy: 0.7326\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4884 - accuracy: 0.7732 - val_loss: 0.5531 - val_accuracy: 0.7326\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4927 - accuracy: 0.7732 - val_loss: 0.5703 - val_accuracy: 0.7326\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4883 - accuracy: 0.7732 - val_loss: 0.5742 - val_accuracy: 0.7326\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4936 - accuracy: 0.7732 - val_loss: 0.5665 - val_accuracy: 0.7326\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4879 - accuracy: 0.7732 - val_loss: 0.5639 - val_accuracy: 0.7326\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4857 - accuracy: 0.7732 - val_loss: 0.5550 - val_accuracy: 0.7326\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4930 - accuracy: 0.7732 - val_loss: 0.5680 - val_accuracy: 0.7326\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4873 - accuracy: 0.7732 - val_loss: 0.5592 - val_accuracy: 0.7326\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4900 - accuracy: 0.7732 - val_loss: 0.5534 - val_accuracy: 0.7326\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4901 - accuracy: 0.7732 - val_loss: 0.5618 - val_accuracy: 0.7326\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4883 - accuracy: 0.7732 - val_loss: 0.5598 - val_accuracy: 0.7326\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4875 - accuracy: 0.7732 - val_loss: 0.5559 - val_accuracy: 0.7326\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4868 - accuracy: 0.7732 - val_loss: 0.5681 - val_accuracy: 0.7326\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4853 - accuracy: 0.7732 - val_loss: 0.5552 - val_accuracy: 0.7326\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4882 - accuracy: 0.7732 - val_loss: 0.5661 - val_accuracy: 0.7326\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4863 - accuracy: 0.7732 - val_loss: 0.5552 - val_accuracy: 0.7326\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4837 - accuracy: 0.7732 - val_loss: 0.5988 - val_accuracy: 0.7326\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.5076 - accuracy: 0.7732 - val_loss: 0.5642 - val_accuracy: 0.7326\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5328 - accuracy: 0.7732 - val_loss: 0.5518 - val_accuracy: 0.7326\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5057 - accuracy: 0.7732 - val_loss: 0.5953 - val_accuracy: 0.7326\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4918 - accuracy: 0.7732 - val_loss: 0.5486 - val_accuracy: 0.7326\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4839 - accuracy: 0.7732 - val_loss: 0.6223 - val_accuracy: 0.7326\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5016 - accuracy: 0.7732 - val_loss: 0.5541 - val_accuracy: 0.7326\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5035 - accuracy: 0.7732 - val_loss: 0.5484 - val_accuracy: 0.7326\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4997 - accuracy: 0.7732 - val_loss: 0.5550 - val_accuracy: 0.7326\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4889 - accuracy: 0.7732 - val_loss: 0.5635 - val_accuracy: 0.7326\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.7732 - val_loss: 0.5708 - val_accuracy: 0.7326\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4914 - accuracy: 0.7732 - val_loss: 0.5756 - val_accuracy: 0.7326\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4907 - accuracy: 0.7732 - val_loss: 0.5505 - val_accuracy: 0.7326\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4937 - accuracy: 0.7732 - val_loss: 0.5545 - val_accuracy: 0.7326\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4926 - accuracy: 0.7732 - val_loss: 0.5698 - val_accuracy: 0.7326\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4998 - accuracy: 0.7732 - val_loss: 0.5486 - val_accuracy: 0.7326\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5037 - accuracy: 0.7732 - val_loss: 0.5692 - val_accuracy: 0.7326\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4879 - accuracy: 0.7732 - val_loss: 0.5773 - val_accuracy: 0.7326\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4923 - accuracy: 0.7732 - val_loss: 0.5495 - val_accuracy: 0.7326\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5019 - accuracy: 0.7732 - val_loss: 0.5594 - val_accuracy: 0.7326\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5010 - accuracy: 0.7732 - val_loss: 0.5482 - val_accuracy: 0.7326\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4970 - accuracy: 0.7732 - val_loss: 0.5645 - val_accuracy: 0.7326\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4970 - accuracy: 0.7732 - val_loss: 0.5479 - val_accuracy: 0.7326\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5006 - accuracy: 0.7732 - val_loss: 0.5629 - val_accuracy: 0.7326\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.7732 - val_loss: 0.5734 - val_accuracy: 0.7326\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4821 - accuracy: 0.7732 - val_loss: 0.5480 - val_accuracy: 0.7326\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5128 - accuracy: 0.7732 - val_loss: 0.5480 - val_accuracy: 0.7326\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4896 - accuracy: 0.7732 - val_loss: 0.5692 - val_accuracy: 0.7326\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4882 - accuracy: 0.7732 - val_loss: 0.5547 - val_accuracy: 0.7326\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4931 - accuracy: 0.7732 - val_loss: 0.5539 - val_accuracy: 0.7326\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.7732 - val_loss: 0.5694 - val_accuracy: 0.7326\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.7732 - val_loss: 0.5677 - val_accuracy: 0.7326\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4923 - accuracy: 0.7732 - val_loss: 0.5480 - val_accuracy: 0.7326\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5065 - accuracy: 0.7732 - val_loss: 0.5499 - val_accuracy: 0.7326\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4807 - accuracy: 0.7732 - val_loss: 0.6078 - val_accuracy: 0.7326\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5147 - accuracy: 0.7732 - val_loss: 0.5478 - val_accuracy: 0.7326\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4942 - accuracy: 0.7732 - val_loss: 0.5885 - val_accuracy: 0.7326\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4876 - accuracy: 0.7732 - val_loss: 0.5524 - val_accuracy: 0.7326\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4853 - accuracy: 0.7732 - val_loss: 0.5774 - val_accuracy: 0.7326\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.7732 - val_loss: 0.5476 - val_accuracy: 0.7326\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4964 - accuracy: 0.7732 - val_loss: 0.5659 - val_accuracy: 0.7326\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4886 - accuracy: 0.7732 - val_loss: 0.5498 - val_accuracy: 0.7326\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.7732 - val_loss: 0.5711 - val_accuracy: 0.7326\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4858 - accuracy: 0.7732 - val_loss: 0.5683 - val_accuracy: 0.7326\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4895 - accuracy: 0.7732 - val_loss: 0.5523 - val_accuracy: 0.7326\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4888 - accuracy: 0.7732 - val_loss: 0.5733 - val_accuracy: 0.7326\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4837 - accuracy: 0.7732 - val_loss: 0.5539 - val_accuracy: 0.7326\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4869 - accuracy: 0.7732 - val_loss: 0.5842 - val_accuracy: 0.7326\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4832 - accuracy: 0.7732 - val_loss: 0.5532 - val_accuracy: 0.7326\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4868 - accuracy: 0.7732 - val_loss: 0.5732 - val_accuracy: 0.7326\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4881 - accuracy: 0.7732 - val_loss: 0.5673 - val_accuracy: 0.7326\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4864 - accuracy: 0.7732 - val_loss: 0.5630 - val_accuracy: 0.7326\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.7732 - val_loss: 0.5539 - val_accuracy: 0.7326\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4858 - accuracy: 0.7732 - val_loss: 0.5979 - val_accuracy: 0.7326\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4917 - accuracy: 0.7732 - val_loss: 0.5506 - val_accuracy: 0.7326\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4945 - accuracy: 0.7732 - val_loss: 0.5494 - val_accuracy: 0.7326\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5074 - accuracy: 0.7732 - val_loss: 0.5474 - val_accuracy: 0.7326\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4851 - accuracy: 0.7732 - val_loss: 0.5712 - val_accuracy: 0.7326\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4928 - accuracy: 0.7732 - val_loss: 0.5489 - val_accuracy: 0.7326\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4928 - accuracy: 0.7732 - val_loss: 0.5494 - val_accuracy: 0.7326\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5136 - accuracy: 0.7732 - val_loss: 0.5470 - val_accuracy: 0.7326\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4929 - accuracy: 0.7732 - val_loss: 0.5769 - val_accuracy: 0.7326\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4865 - accuracy: 0.7732 - val_loss: 0.5644 - val_accuracy: 0.7326\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4847 - accuracy: 0.7732 - val_loss: 0.5502 - val_accuracy: 0.7326\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4920 - accuracy: 0.7732 - val_loss: 0.5839 - val_accuracy: 0.7326\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7732 - val_loss: 0.5496 - val_accuracy: 0.7326\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4890 - accuracy: 0.7732 - val_loss: 0.5802 - val_accuracy: 0.7326\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4934 - accuracy: 0.7732 - val_loss: 0.5535 - val_accuracy: 0.7326\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4847 - accuracy: 0.7732 - val_loss: 0.5567 - val_accuracy: 0.7326\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4855 - accuracy: 0.7732 - val_loss: 0.5550 - val_accuracy: 0.7326\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4871 - accuracy: 0.7732 - val_loss: 0.5501 - val_accuracy: 0.7326\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5007 - accuracy: 0.7732 - val_loss: 0.5654 - val_accuracy: 0.7326\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4854 - accuracy: 0.7732 - val_loss: 0.5484 - val_accuracy: 0.7326\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4919 - accuracy: 0.7732 - val_loss: 0.5701 - val_accuracy: 0.7326\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.7732 - val_loss: 0.5594 - val_accuracy: 0.7326\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4889 - accuracy: 0.7732 - val_loss: 0.5701 - val_accuracy: 0.7326\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4881 - accuracy: 0.7732 - val_loss: 0.5488 - val_accuracy: 0.7326\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4844 - accuracy: 0.7732 - val_loss: 0.5734 - val_accuracy: 0.7326\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4889 - accuracy: 0.7732 - val_loss: 0.5466 - val_accuracy: 0.7326\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.7732 - val_loss: 0.5610 - val_accuracy: 0.7326\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.7732 - val_loss: 0.5703 - val_accuracy: 0.7326\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4812 - accuracy: 0.7732 - val_loss: 0.5504 - val_accuracy: 0.7326\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4946 - accuracy: 0.7732 - val_loss: 0.5488 - val_accuracy: 0.7326\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4897 - accuracy: 0.7732 - val_loss: 0.5519 - val_accuracy: 0.7326\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4861 - accuracy: 0.7732 - val_loss: 0.5618 - val_accuracy: 0.7326\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4951 - accuracy: 0.7732 - val_loss: 0.5465 - val_accuracy: 0.7326\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4898 - accuracy: 0.7732 - val_loss: 0.5832 - val_accuracy: 0.7326\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4892 - accuracy: 0.7732 - val_loss: 0.5520 - val_accuracy: 0.7326\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4856 - accuracy: 0.7732 - val_loss: 0.5554 - val_accuracy: 0.7326\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.7732 - val_loss: 0.5527 - val_accuracy: 0.7326\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.7732 - val_loss: 0.5668 - val_accuracy: 0.7326\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4843 - accuracy: 0.7732 - val_loss: 0.5505 - val_accuracy: 0.7326\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.7732 - val_loss: 0.5764 - val_accuracy: 0.7326\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4957 - accuracy: 0.7732 - val_loss: 0.5496 - val_accuracy: 0.7326\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4843 - accuracy: 0.7732 - val_loss: 0.5570 - val_accuracy: 0.7326\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4853 - accuracy: 0.7732 - val_loss: 0.5675 - val_accuracy: 0.7326\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4869 - accuracy: 0.7732 - val_loss: 0.5461 - val_accuracy: 0.7326\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5010 - accuracy: 0.7732 - val_loss: 0.5634 - val_accuracy: 0.7326\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4825 - accuracy: 0.7732 - val_loss: 0.5669 - val_accuracy: 0.7326\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4855 - accuracy: 0.7732 - val_loss: 0.5520 - val_accuracy: 0.7326\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4847 - accuracy: 0.7732 - val_loss: 0.5659 - val_accuracy: 0.7326\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4880 - accuracy: 0.7732 - val_loss: 0.5479 - val_accuracy: 0.7326\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4830 - accuracy: 0.7732 - val_loss: 0.5712 - val_accuracy: 0.7326\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.7732 - val_loss: 0.5609 - val_accuracy: 0.7326\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4819 - accuracy: 0.7732 - val_loss: 0.5482 - val_accuracy: 0.7326\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4902 - accuracy: 0.7732 - val_loss: 0.5493 - val_accuracy: 0.7326\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4820 - accuracy: 0.7732 - val_loss: 0.5721 - val_accuracy: 0.7326\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4834 - accuracy: 0.7732 - val_loss: 0.5588 - val_accuracy: 0.7326\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4835 - accuracy: 0.7732 - val_loss: 0.5744 - val_accuracy: 0.7326\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5086 - accuracy: 0.7732 - val_loss: 0.5551 - val_accuracy: 0.7326\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5157 - accuracy: 0.7732 - val_loss: 0.5474 - val_accuracy: 0.7326\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4861 - accuracy: 0.7732 - val_loss: 0.5456 - val_accuracy: 0.7326\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4915 - accuracy: 0.7732 - val_loss: 0.5678 - val_accuracy: 0.7326\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4914 - accuracy: 0.7732 - val_loss: 0.5452 - val_accuracy: 0.7326\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4901 - accuracy: 0.7732 - val_loss: 0.5544 - val_accuracy: 0.7326\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4829 - accuracy: 0.7732 - val_loss: 0.5562 - val_accuracy: 0.7326\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4835 - accuracy: 0.7732 - val_loss: 0.5525 - val_accuracy: 0.7326\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4857 - accuracy: 0.7732 - val_loss: 0.5657 - val_accuracy: 0.7326\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4841 - accuracy: 0.7732 - val_loss: 0.5448 - val_accuracy: 0.7326\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4945 - accuracy: 0.7732 - val_loss: 0.5496 - val_accuracy: 0.7326\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4893 - accuracy: 0.7732 - val_loss: 0.5699 - val_accuracy: 0.7326\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4807 - accuracy: 0.7732 - val_loss: 0.5445 - val_accuracy: 0.7326\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4915 - accuracy: 0.7732 - val_loss: 0.5796 - val_accuracy: 0.7326\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5067 - accuracy: 0.7732 - val_loss: 0.5456 - val_accuracy: 0.7326\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5006 - accuracy: 0.7732 - val_loss: 0.5512 - val_accuracy: 0.7326\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4851 - accuracy: 0.7732 - val_loss: 0.5453 - val_accuracy: 0.7326\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5048 - accuracy: 0.7732 - val_loss: 0.5448 - val_accuracy: 0.7326\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4874 - accuracy: 0.7732 - val_loss: 0.5815 - val_accuracy: 0.7326\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4835 - accuracy: 0.7732 - val_loss: 0.5565 - val_accuracy: 0.7326\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4957 - accuracy: 0.7732 - val_loss: 0.5452 - val_accuracy: 0.7326\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4928 - accuracy: 0.7732 - val_loss: 0.5751 - val_accuracy: 0.7326\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4854 - accuracy: 0.7732 - val_loss: 0.5501 - val_accuracy: 0.7326\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4820 - accuracy: 0.7732 - val_loss: 0.5678 - val_accuracy: 0.7326\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4928 - accuracy: 0.7732 - val_loss: 0.5494 - val_accuracy: 0.7326\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5143 - accuracy: 0.7732 - val_loss: 0.5442 - val_accuracy: 0.7326\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4913 - accuracy: 0.7732 - val_loss: 0.5698 - val_accuracy: 0.7326\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4864 - accuracy: 0.7732 - val_loss: 0.5669 - val_accuracy: 0.7326\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4873 - accuracy: 0.7732 - val_loss: 0.5505 - val_accuracy: 0.7326\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4861 - accuracy: 0.7732 - val_loss: 0.5506 - val_accuracy: 0.7326\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4919 - accuracy: 0.7732 - val_loss: 0.5569 - val_accuracy: 0.7326\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4899 - accuracy: 0.7732 - val_loss: 0.5648 - val_accuracy: 0.7326\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4834 - accuracy: 0.7732 - val_loss: 0.5688 - val_accuracy: 0.7326\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4865 - accuracy: 0.7732 - val_loss: 0.5511 - val_accuracy: 0.7326\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4797 - accuracy: 0.7732 - val_loss: 0.5989 - val_accuracy: 0.7326\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.7732 - val_loss: 0.5478 - val_accuracy: 0.7326\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4863 - accuracy: 0.7732 - val_loss: 0.5704 - val_accuracy: 0.7326\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.7732 - val_loss: 0.5648 - val_accuracy: 0.7326\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.7732 - val_loss: 0.5578 - val_accuracy: 0.7326\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4837 - accuracy: 0.7732 - val_loss: 0.5476 - val_accuracy: 0.7326\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4828 - accuracy: 0.7732 - val_loss: 0.5598 - val_accuracy: 0.7326\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4801 - accuracy: 0.7732 - val_loss: 0.5837 - val_accuracy: 0.7326\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4891 - accuracy: 0.7732 - val_loss: 0.5518 - val_accuracy: 0.7326\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4835 - accuracy: 0.7732 - val_loss: 0.5826 - val_accuracy: 0.7326\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4760 - accuracy: 0.7732 - val_loss: 0.5467 - val_accuracy: 0.7326\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5066 - accuracy: 0.7732 - val_loss: 0.5457 - val_accuracy: 0.7326\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4943 - accuracy: 0.7732 - val_loss: 0.5697 - val_accuracy: 0.7326\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4827 - accuracy: 0.7732 - val_loss: 0.5555 - val_accuracy: 0.7326\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4898 - accuracy: 0.7732 - val_loss: 0.5471 - val_accuracy: 0.7326\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4885 - accuracy: 0.7732 - val_loss: 0.5543 - val_accuracy: 0.7326\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4830 - accuracy: 0.7732 - val_loss: 0.5624 - val_accuracy: 0.7326\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4819 - accuracy: 0.7732 - val_loss: 0.5465 - val_accuracy: 0.7326\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.7732 - val_loss: 0.5646 - val_accuracy: 0.7326\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.7732 - val_loss: 0.5577 - val_accuracy: 0.7326\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.7732 - val_loss: 0.5491 - val_accuracy: 0.7326\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4887 - accuracy: 0.7732 - val_loss: 0.5456 - val_accuracy: 0.7326\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4838 - accuracy: 0.7732 - val_loss: 0.6243 - val_accuracy: 0.7326\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4959 - accuracy: 0.7732 - val_loss: 0.5444 - val_accuracy: 0.7326\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4786 - accuracy: 0.7732 - val_loss: 0.5666 - val_accuracy: 0.7326\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4789 - accuracy: 0.7732 - val_loss: 0.5451 - val_accuracy: 0.7326\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4982 - accuracy: 0.7732 - val_loss: 0.5478 - val_accuracy: 0.7326\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4824 - accuracy: 0.7732 - val_loss: 0.5493 - val_accuracy: 0.7326\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4840 - accuracy: 0.7732 - val_loss: 0.5653 - val_accuracy: 0.7326\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4806 - accuracy: 0.7732 - val_loss: 0.5496 - val_accuracy: 0.7326\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4886 - accuracy: 0.7732 - val_loss: 0.5640 - val_accuracy: 0.7326\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.7732 - val_loss: 0.5801 - val_accuracy: 0.7326\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4893 - accuracy: 0.7732 - val_loss: 0.5602 - val_accuracy: 0.7326\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4800 - accuracy: 0.7732 - val_loss: 0.5621 - val_accuracy: 0.7326\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4812 - accuracy: 0.7732 - val_loss: 0.5574 - val_accuracy: 0.7326\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4801 - accuracy: 0.7732 - val_loss: 0.5507 - val_accuracy: 0.7326\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4768 - accuracy: 0.7732 - val_loss: 0.5938 - val_accuracy: 0.7326\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4999 - accuracy: 0.7732 - val_loss: 0.5449 - val_accuracy: 0.7326\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4848 - accuracy: 0.7732 - val_loss: 0.6198 - val_accuracy: 0.7326\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4834 - accuracy: 0.7732 - val_loss: 0.5435 - val_accuracy: 0.7326\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4919 - accuracy: 0.7732 - val_loss: 0.5816 - val_accuracy: 0.7326\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5033 - accuracy: 0.7732 - val_loss: 0.5536 - val_accuracy: 0.7326\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5235 - accuracy: 0.7732 - val_loss: 0.5437 - val_accuracy: 0.7326\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4912 - accuracy: 0.7732 - val_loss: 0.5592 - val_accuracy: 0.7326\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4842 - accuracy: 0.7732 - val_loss: 0.5712 - val_accuracy: 0.7326\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4846 - accuracy: 0.7732 - val_loss: 0.5561 - val_accuracy: 0.7326\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4829 - accuracy: 0.7732 - val_loss: 0.5463 - val_accuracy: 0.7326\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4818 - accuracy: 0.7732 - val_loss: 0.5871 - val_accuracy: 0.7326\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4768 - accuracy: 0.7732 - val_loss: 0.5448 - val_accuracy: 0.7326\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4809 - accuracy: 0.7732 - val_loss: 0.5720 - val_accuracy: 0.7326\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4968 - accuracy: 0.7732 - val_loss: 0.5420 - val_accuracy: 0.7326\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4823 - accuracy: 0.7732 - val_loss: 0.5910 - val_accuracy: 0.7326\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4929 - accuracy: 0.7732 - val_loss: 0.5459 - val_accuracy: 0.7326\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4767 - accuracy: 0.7732 - val_loss: 0.5685 - val_accuracy: 0.7326\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4846 - accuracy: 0.7732 - val_loss: 0.5598 - val_accuracy: 0.7326\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4857 - accuracy: 0.7732 - val_loss: 0.5485 - val_accuracy: 0.7326\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4823 - accuracy: 0.7732 - val_loss: 0.5456 - val_accuracy: 0.7326\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4901 - accuracy: 0.7732 - val_loss: 0.5505 - val_accuracy: 0.7326\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4785 - accuracy: 0.7732 - val_loss: 0.5496 - val_accuracy: 0.7326\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4831 - accuracy: 0.7732 - val_loss: 0.5635 - val_accuracy: 0.7326\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4874 - accuracy: 0.7732 - val_loss: 0.5463 - val_accuracy: 0.7326\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5015 - accuracy: 0.7732 - val_loss: 0.5426 - val_accuracy: 0.7326\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4954 - accuracy: 0.7732 - val_loss: 0.5500 - val_accuracy: 0.7326\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4829 - accuracy: 0.7732 - val_loss: 0.5502 - val_accuracy: 0.7326\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4840 - accuracy: 0.7732 - val_loss: 0.5459 - val_accuracy: 0.7326\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4807 - accuracy: 0.7732 - val_loss: 0.5469 - val_accuracy: 0.7326\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4840 - accuracy: 0.7732 - val_loss: 0.5715 - val_accuracy: 0.7326\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4835 - accuracy: 0.7732 - val_loss: 0.5552 - val_accuracy: 0.7326\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4762 - accuracy: 0.7732 - val_loss: 0.5709 - val_accuracy: 0.7326\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4820 - accuracy: 0.7732 - val_loss: 0.5455 - val_accuracy: 0.7326\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4790 - accuracy: 0.7732 - val_loss: 0.5445 - val_accuracy: 0.7326\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4857 - accuracy: 0.7732 - val_loss: 0.5616 - val_accuracy: 0.7326\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.7732 - val_loss: 0.5457 - val_accuracy: 0.7326\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5032 - accuracy: 0.7732 - val_loss: 0.5417 - val_accuracy: 0.7326\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4789 - accuracy: 0.7732 - val_loss: 0.5840 - val_accuracy: 0.7326\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4792 - accuracy: 0.7732 - val_loss: 0.5545 - val_accuracy: 0.7326\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4839 - accuracy: 0.7732 - val_loss: 0.5447 - val_accuracy: 0.7326\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4783 - accuracy: 0.7732 - val_loss: 0.5642 - val_accuracy: 0.7326\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4789 - accuracy: 0.7732 - val_loss: 0.5462 - val_accuracy: 0.7326\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4868 - accuracy: 0.7732 - val_loss: 0.5440 - val_accuracy: 0.7326\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4834 - accuracy: 0.7732 - val_loss: 0.5653 - val_accuracy: 0.7326\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.7732 - val_loss: 0.5790 - val_accuracy: 0.7326\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4941 - accuracy: 0.7732 - val_loss: 0.5447 - val_accuracy: 0.7326\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4840 - accuracy: 0.7732 - val_loss: 0.5509 - val_accuracy: 0.7326\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7732 - val_loss: 0.5717 - val_accuracy: 0.7326\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4820 - accuracy: 0.7732 - val_loss: 0.5441 - val_accuracy: 0.7326\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4816 - accuracy: 0.7732 - val_loss: 0.5642 - val_accuracy: 0.7326\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4778 - accuracy: 0.7732 - val_loss: 0.5486 - val_accuracy: 0.7326\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4758 - accuracy: 0.7732 - val_loss: 0.5638 - val_accuracy: 0.7326\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4756 - accuracy: 0.7732 - val_loss: 0.5456 - val_accuracy: 0.7326\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4832 - accuracy: 0.7732 - val_loss: 0.5524 - val_accuracy: 0.7326\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4819 - accuracy: 0.7732 - val_loss: 0.5612 - val_accuracy: 0.7326\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4868 - accuracy: 0.7732 - val_loss: 0.5491 - val_accuracy: 0.7326\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4896 - accuracy: 0.7732 - val_loss: 0.5451 - val_accuracy: 0.7326\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4902 - accuracy: 0.7732 - val_loss: 0.5405 - val_accuracy: 0.7326\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4871 - accuracy: 0.7732 - val_loss: 0.5693 - val_accuracy: 0.7326\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4764 - accuracy: 0.7732 - val_loss: 0.5457 - val_accuracy: 0.7326\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4803 - accuracy: 0.7732 - val_loss: 0.5614 - val_accuracy: 0.7326\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4790 - accuracy: 0.7732 - val_loss: 0.5463 - val_accuracy: 0.7326\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4774 - accuracy: 0.7732 - val_loss: 0.5871 - val_accuracy: 0.7326\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.5085 - accuracy: 0.7732 - val_loss: 0.5431 - val_accuracy: 0.7326\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4884 - accuracy: 0.7732 - val_loss: 0.5632 - val_accuracy: 0.7326\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4802 - accuracy: 0.7732 - val_loss: 0.5468 - val_accuracy: 0.7326\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4842 - accuracy: 0.7732 - val_loss: 0.5435 - val_accuracy: 0.7326\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4816 - accuracy: 0.7732 - val_loss: 0.5624 - val_accuracy: 0.7326\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4787 - accuracy: 0.7732 - val_loss: 0.5442 - val_accuracy: 0.7326\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4797 - accuracy: 0.7732 - val_loss: 0.5693 - val_accuracy: 0.7326\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4764 - accuracy: 0.7732 - val_loss: 0.5611 - val_accuracy: 0.7326\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4798 - accuracy: 0.7732 - val_loss: 0.5561 - val_accuracy: 0.7326\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4778 - accuracy: 0.7732 - val_loss: 0.5445 - val_accuracy: 0.7326\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4802 - accuracy: 0.7732 - val_loss: 0.5450 - val_accuracy: 0.7326\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4784 - accuracy: 0.7732 - val_loss: 0.5675 - val_accuracy: 0.7326\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4812 - accuracy: 0.7732 - val_loss: 0.5444 - val_accuracy: 0.7326\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4928 - accuracy: 0.7732 - val_loss: 0.5556 - val_accuracy: 0.7326\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4830 - accuracy: 0.7732 - val_loss: 0.5574 - val_accuracy: 0.7326\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4804 - accuracy: 0.7732 - val_loss: 0.5556 - val_accuracy: 0.7326\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4788 - accuracy: 0.7732 - val_loss: 0.5530 - val_accuracy: 0.7326\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4774 - accuracy: 0.7732 - val_loss: 0.5588 - val_accuracy: 0.7326\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4891 - accuracy: 0.7732 - val_loss: 0.5911 - val_accuracy: 0.7326\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4769 - accuracy: 0.7732 - val_loss: 0.5462 - val_accuracy: 0.7326\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4772 - accuracy: 0.7732 - val_loss: 0.5566 - val_accuracy: 0.7326\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4770 - accuracy: 0.7732 - val_loss: 0.5436 - val_accuracy: 0.7326\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4801 - accuracy: 0.7732 - val_loss: 0.5661 - val_accuracy: 0.7326\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4757 - accuracy: 0.7732 - val_loss: 0.5529 - val_accuracy: 0.7326\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4875 - accuracy: 0.7732 - val_loss: 0.5431 - val_accuracy: 0.7326\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4841 - accuracy: 0.7732 - val_loss: 0.5603 - val_accuracy: 0.7326\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4774 - accuracy: 0.7732 - val_loss: 0.5574 - val_accuracy: 0.7326\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4798 - accuracy: 0.7732 - val_loss: 0.5432 - val_accuracy: 0.7326\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4908 - accuracy: 0.7732 - val_loss: 0.5417 - val_accuracy: 0.7326\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5064 - accuracy: 0.7732 - val_loss: 0.5415 - val_accuracy: 0.7326\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4754 - accuracy: 0.7732 - val_loss: 0.5647 - val_accuracy: 0.7326\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4782 - accuracy: 0.7732 - val_loss: 0.5436 - val_accuracy: 0.7326\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4808 - accuracy: 0.7732 - val_loss: 0.5774 - val_accuracy: 0.7326\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4977 - accuracy: 0.7732 - val_loss: 0.5399 - val_accuracy: 0.7326\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4823 - accuracy: 0.7732 - val_loss: 0.5602 - val_accuracy: 0.7326\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4745 - accuracy: 0.7732 - val_loss: 0.5601 - val_accuracy: 0.7326\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4729 - accuracy: 0.7732 - val_loss: 0.5405 - val_accuracy: 0.7326\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5013 - accuracy: 0.7732 - val_loss: 0.5433 - val_accuracy: 0.7326\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4834 - accuracy: 0.7732 - val_loss: 0.5489 - val_accuracy: 0.7326\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4977 - accuracy: 0.7732 - val_loss: 0.5380 - val_accuracy: 0.7326\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4898 - accuracy: 0.7732 - val_loss: 0.5550 - val_accuracy: 0.7326\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4810 - accuracy: 0.7732 - val_loss: 0.5490 - val_accuracy: 0.7326\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4833 - accuracy: 0.7732 - val_loss: 0.5414 - val_accuracy: 0.7326\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4802 - accuracy: 0.7732 - val_loss: 0.5702 - val_accuracy: 0.7326\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4798 - accuracy: 0.7732 - val_loss: 0.5470 - val_accuracy: 0.7326\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4754 - accuracy: 0.7732 - val_loss: 0.5514 - val_accuracy: 0.7326\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4790 - accuracy: 0.7732 - val_loss: 0.5473 - val_accuracy: 0.7326\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4779 - accuracy: 0.7732 - val_loss: 0.5704 - val_accuracy: 0.7326\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4800 - accuracy: 0.7732 - val_loss: 0.5448 - val_accuracy: 0.7326\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4849 - accuracy: 0.7732 - val_loss: 0.5390 - val_accuracy: 0.7326\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4940 - accuracy: 0.7732 - val_loss: 0.5450 - val_accuracy: 0.7326\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4975 - accuracy: 0.7732 - val_loss: 0.5416 - val_accuracy: 0.7326\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5015 - accuracy: 0.7732 - val_loss: 0.5393 - val_accuracy: 0.7326\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4863 - accuracy: 0.7732 - val_loss: 0.5641 - val_accuracy: 0.7326\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.7732 - val_loss: 0.5450 - val_accuracy: 0.7326\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4771 - accuracy: 0.7732 - val_loss: 0.5518 - val_accuracy: 0.7326\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4857 - accuracy: 0.7732 - val_loss: 0.5463 - val_accuracy: 0.7326\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4871 - accuracy: 0.7732 - val_loss: 0.5383 - val_accuracy: 0.7326\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4914 - accuracy: 0.7732 - val_loss: 0.5474 - val_accuracy: 0.7326\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4737 - accuracy: 0.7732 - val_loss: 0.5770 - val_accuracy: 0.7326\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4890 - accuracy: 0.7732 - val_loss: 0.5444 - val_accuracy: 0.7326\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4812 - accuracy: 0.7732 - val_loss: 0.5537 - val_accuracy: 0.7326\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4800 - accuracy: 0.7732 - val_loss: 0.5555 - val_accuracy: 0.7326\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4857 - accuracy: 0.7732 - val_loss: 0.5434 - val_accuracy: 0.7326\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4761 - accuracy: 0.7732 - val_loss: 0.5652 - val_accuracy: 0.7326\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4754 - accuracy: 0.7732 - val_loss: 0.5457 - val_accuracy: 0.7326\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4763 - accuracy: 0.7732 - val_loss: 0.5733 - val_accuracy: 0.7326\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.7732 - val_loss: 0.5446 - val_accuracy: 0.7326\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4734 - accuracy: 0.7732 - val_loss: 0.5632 - val_accuracy: 0.7326\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4747 - accuracy: 0.7732 - val_loss: 0.5425 - val_accuracy: 0.7326\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4745 - accuracy: 0.7732 - val_loss: 0.5741 - val_accuracy: 0.7326\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4747 - accuracy: 0.7732 - val_loss: 0.5439 - val_accuracy: 0.7326\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4773 - accuracy: 0.7732 - val_loss: 0.5546 - val_accuracy: 0.7326\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4743 - accuracy: 0.7732 - val_loss: 0.5455 - val_accuracy: 0.7326\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4725 - accuracy: 0.7732 - val_loss: 0.5718 - val_accuracy: 0.7326\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4788 - accuracy: 0.7732 - val_loss: 0.5447 - val_accuracy: 0.7326\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4742 - accuracy: 0.7732 - val_loss: 0.5541 - val_accuracy: 0.7326\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.7732 - val_loss: 0.5531 - val_accuracy: 0.7326\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5060 - accuracy: 0.7732 - val_loss: 0.5422 - val_accuracy: 0.7326\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4955 - accuracy: 0.7732 - val_loss: 0.5721 - val_accuracy: 0.7326\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4826 - accuracy: 0.7732 - val_loss: 0.5408 - val_accuracy: 0.7326\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4784 - accuracy: 0.7732 - val_loss: 0.5682 - val_accuracy: 0.7326\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4787 - accuracy: 0.7732 - val_loss: 0.5457 - val_accuracy: 0.7326\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4768 - accuracy: 0.7732 - val_loss: 0.5658 - val_accuracy: 0.7326\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4912 - accuracy: 0.7732 - val_loss: 0.5375 - val_accuracy: 0.7326\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4870 - accuracy: 0.7732 - val_loss: 0.5821 - val_accuracy: 0.7326\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.7732 - val_loss: 0.5456 - val_accuracy: 0.7326\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4761 - accuracy: 0.7732 - val_loss: 0.5623 - val_accuracy: 0.7326\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4758 - accuracy: 0.7732 - val_loss: 0.5644 - val_accuracy: 0.7326\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4821 - accuracy: 0.7732 - val_loss: 0.5430 - val_accuracy: 0.7326\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4808 - accuracy: 0.7732 - val_loss: 0.5642 - val_accuracy: 0.7326\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.4759 - accuracy: 0.7732 - val_loss: 0.5470 - val_accuracy: 0.7326\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4782 - accuracy: 0.7732 - val_loss: 0.5466 - val_accuracy: 0.7326\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4790 - accuracy: 0.7732 - val_loss: 0.5631 - val_accuracy: 0.7326\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4737 - accuracy: 0.7732 - val_loss: 0.5428 - val_accuracy: 0.7326\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.4798 - accuracy: 0.7732 - val_loss: 0.5587 - val_accuracy: 0.7326\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4751 - accuracy: 0.7732 - val_loss: 0.5434 - val_accuracy: 0.7326\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4808 - accuracy: 0.7732 - val_loss: 0.5580 - val_accuracy: 0.7326\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 19ms/step - loss: 0.4773 - accuracy: 0.7732 - val_loss: 0.5628 - val_accuracy: 0.7326\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.4784 - accuracy: 0.7732 - val_loss: 0.5498 - val_accuracy: 0.7326\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.4748 - accuracy: 0.7732 - val_loss: 0.5635 - val_accuracy: 0.7326\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.4871 - accuracy: 0.7732 - val_loss: 0.5416 - val_accuracy: 0.7326\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.4764 - accuracy: 0.7732 - val_loss: 0.5601 - val_accuracy: 0.7326\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.4825 - accuracy: 0.7732 - val_loss: 0.5434 - val_accuracy: 0.7326\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4975 - accuracy: 0.7732 - val_loss: 0.5628 - val_accuracy: 0.7326\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4802 - accuracy: 0.7732 - val_loss: 0.5529 - val_accuracy: 0.7326\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4809 - accuracy: 0.7732 - val_loss: 0.5418 - val_accuracy: 0.7326\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5131 - accuracy: 0.7732 - val_loss: 0.5389 - val_accuracy: 0.7326\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4817 - accuracy: 0.7732 - val_loss: 0.5819 - val_accuracy: 0.7326\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4911 - accuracy: 0.7732 - val_loss: 0.5388 - val_accuracy: 0.7326\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4838 - accuracy: 0.7732 - val_loss: 0.5498 - val_accuracy: 0.7326\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4769 - accuracy: 0.7732 - val_loss: 0.5644 - val_accuracy: 0.7326\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4797 - accuracy: 0.7732 - val_loss: 0.5603 - val_accuracy: 0.7326\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4849 - accuracy: 0.7732 - val_loss: 0.5419 - val_accuracy: 0.7326\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4995 - accuracy: 0.7732 - val_loss: 0.5395 - val_accuracy: 0.7326\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4788 - accuracy: 0.7732 - val_loss: 0.5439 - val_accuracy: 0.7326\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4955 - accuracy: 0.7732 - val_loss: 0.5365 - val_accuracy: 0.7326\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4863 - accuracy: 0.7732 - val_loss: 0.5528 - val_accuracy: 0.7326\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4761 - accuracy: 0.7732 - val_loss: 0.5485 - val_accuracy: 0.7326\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4783 - accuracy: 0.7732 - val_loss: 0.5425 - val_accuracy: 0.7326\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4821 - accuracy: 0.7732 - val_loss: 0.5711 - val_accuracy: 0.7326\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4729 - accuracy: 0.7732 - val_loss: 0.5423 - val_accuracy: 0.7326\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4754 - accuracy: 0.7732 - val_loss: 0.5596 - val_accuracy: 0.7326\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4803 - accuracy: 0.7732 - val_loss: 0.5454 - val_accuracy: 0.7326\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.4805 - accuracy: 0.7732 - val_loss: 0.5474 - val_accuracy: 0.7326\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4805 - accuracy: 0.7732 - val_loss: 0.5517 - val_accuracy: 0.7326\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4743 - accuracy: 0.7732 - val_loss: 0.5534 - val_accuracy: 0.7326\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4787 - accuracy: 0.7732 - val_loss: 0.5430 - val_accuracy: 0.7326\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4801 - accuracy: 0.7732 - val_loss: 0.5695 - val_accuracy: 0.7326\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4829 - accuracy: 0.7732 - val_loss: 0.5427 - val_accuracy: 0.7326\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4723 - accuracy: 0.7732 - val_loss: 0.5571 - val_accuracy: 0.7326\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4741 - accuracy: 0.7732 - val_loss: 0.5619 - val_accuracy: 0.7326\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.7732 - val_loss: 0.5550 - val_accuracy: 0.7326\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4882 - accuracy: 0.7732 - val_loss: 0.5401 - val_accuracy: 0.7326\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4908 - accuracy: 0.7732 - val_loss: 0.5614 - val_accuracy: 0.7326\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4866 - accuracy: 0.7732 - val_loss: 0.5428 - val_accuracy: 0.7326\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5428 - accuracy: 0.7326\n",
      "Testing Accuracy: 0.7326202988624573\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.85       137\n",
      "           1       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.73       187\n",
      "   macro avg       0.37      0.50      0.42       187\n",
      "weighted avg       0.54      0.73      0.62       187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess your data (replace with your data loading code)\n",
    "data = pd.read_csv('/content/drive/MyDrive/AI ML/blood-tranfer.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = np.array(data)[:, 1:-1]\n",
    "Y = np.array(data)[:, -1]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(11, activation='relu'),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(Y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print(\"Testing Accuracy: {}\".format(accuracy[1]))\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_result = model.predict(X_test)\n",
    "Y_result = np.argmax(Y_result, axis=1)\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "classification_rep = classification_report(Y_test, Y_result)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Calculate fpr and tpr\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_result)\n",
    "\n",
    "# Plot ROC curve\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "8iz8wqZxGzLy",
    "outputId": "3af3084c-8aba-4753-8abc-556aab4da1fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "<ipython-input-70-86acf1745a9b>:32: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-86acf1745a9b>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# Train the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Accuracy: {:.2f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-86acf1745a9b>\u001b[0m in \u001b[0;36mneural_network\u001b[0;34m(X_train, Y_train, X_val, Y_val, epochs, nodes, lr)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-86acf1745a9b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, lr, weights)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-86acf1745a9b>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(y, activations, weights, layers, lr)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0moutput_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (1,5) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, classification_report, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset (Assuming it's a CSV file)\n",
    "data = pd.read_csv('/content/drive/MyDrive/AI ML/blood-tranfer.csv')\n",
    "\n",
    "# You may want to shuffle your data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Assuming you want to use all columns except the last one as features\n",
    "X = np.array(data)[:, :-1]\n",
    "\n",
    "# Assuming the last column is the target\n",
    "Y = np.array(data)[:, -1]\n",
    "\n",
    "# Perform one-hot encoding for multi-class classification\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "Y = Y.reshape(-1, 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "Y = onehot_encoder.fit_transform(Y)\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# Define your neural network\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def neural_network(X_train, Y_train, X_val=None, Y_val=None, epochs=500, nodes=[11, 8, 5, 5], lr=0.1):\n",
    "    num_layers = len(nodes)\n",
    "    layers = [X_train.shape[1]] + nodes + [Y_train.shape[1]]\n",
    "    weights = initialize_weights(layers)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        weights = train(X_train, Y_train, lr, weights)\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "            print(\"Training Accuracy: {:.2f}%\".format(accuracy(X_train, Y_train, weights) * 100))\n",
    "\n",
    "            if X_val is not None:\n",
    "                print(\"Validation Accuracy: {:.2f}%\".format(accuracy(X_val, Y_val, weights) * 100))\n",
    "\n",
    "    return weights\n",
    "\n",
    "def initialize_weights(layers):\n",
    "    num_layers = len(layers) - 1\n",
    "    weights = []\n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        w = np.random.uniform(-1, 1, (layers[i], layers[i - 1] + 1))\n",
    "        weights.append(np.matrix(w))\n",
    "\n",
    "    return weights\n",
    "\n",
    "def forward_propagation(x, weights, layers):\n",
    "    activations = [x]\n",
    "    layer_input = x\n",
    "\n",
    "    for j in range(layers):\n",
    "        activation = sigmoid(np.dot(layer_input, weights[j].T))\n",
    "        activations.append(activation)\n",
    "        layer_input = np.append(1, activation)\n",
    "\n",
    "    return activations\n",
    "\n",
    "def back_propagation(y, activations, weights, layers, lr):\n",
    "    output_final = activations[-1]\n",
    "    error = np.matrix(y - output_final)\n",
    "\n",
    "    for j in range(layers, 0, -1):\n",
    "        curr_activation = activations[j]\n",
    "\n",
    "        if j > 1:\n",
    "            prev_activation = np.append(1, activations[j - 1])\n",
    "        else:\n",
    "            prev_activation = activations[0]\n",
    "\n",
    "        delta = np.multiply(error, sigmoid_derivative(curr_activation))\n",
    "        weights[j - 1] += lr * np.multiply(delta.T, prev_activation)\n",
    "        w = np.delete(weights[j - 1], [0], axis=1)\n",
    "        error = np.dot(delta, w)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def train(X, Y, lr, weights):\n",
    "    layers = len(weights)\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], Y[i]\n",
    "        x = np.matrix(np.append(1, x))\n",
    "        activations = forward_propagation(x, weights, layers)\n",
    "        weights = back_propagation(y, activations, weights, layers, lr)\n",
    "    return weights\n",
    "\n",
    "def predict(item, weights):\n",
    "    item = np.matrix(np.append(1, item))\n",
    "    activations = forward_propagation(item, weights, num_layers)\n",
    "    output_final = activations[-1].A1\n",
    "    m, index = output_final[0], 0\n",
    "\n",
    "    for i in range(1, len(output_final)):\n",
    "        if output_final[i] > m:\n",
    "            m, index = output_final[i], i\n",
    "\n",
    "    y = [0] * len(output_final)\n",
    "    y[index] = 1\n",
    "    return y\n",
    "\n",
    "def accuracy(X, Y, weights):\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], Y[i]\n",
    "        guess = predict(x, weights)\n",
    "\n",
    "        if np.array_equal(y, guess):\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(X)\n",
    "\n",
    "# Train the neural network\n",
    "weights = neural_network(X_train, Y_train, X_test, Y_test, epochs=500, nodes=[11, 8, 5, 5], lr=0.1)\n",
    "\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(accuracy(X_test, Y_test, weights) * 100))\n",
    "\n",
    "# Evaluate your model\n",
    "Y_result = []\n",
    "\n",
    "for x in X_test:\n",
    "    guess = predict(x, weights)\n",
    "    Y_result.append(guess)\n",
    "\n",
    "print(\"R2 score: {:.2f}\".format(r2_score(Y_test, Y_result)))\n",
    "print(classification_report(Y_test, Y_result))\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(Y_test.argmax(axis=1), np.array(Y_result).argmax(axis=1))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CUaBFBUM5Z6",
    "outputId": "f06aae12-6764-4d4b-c9a9-87c0d47b6f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 52.94%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset (Assuming it's a CSV file)\n",
    "data = pd.read_csv('/content/drive/MyDrive/AI ML/blood-tranfer.csv')\n",
    "\n",
    "# Select features and target\n",
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Feature scaling (you can customize this part)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Define your neural network (you can expand this architecture as needed)\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 10  # Number of neurons in the hidden layer\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights_input_hidden = np.random.uniform(size=(input_dim, hidden_dim))\n",
    "bias_hidden = np.zeros((1, hidden_dim))\n",
    "weights_hidden_output = np.random.uniform(size=(hidden_dim, output_dim))\n",
    "bias_output = np.zeros((1, output_dim))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    output_layer_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_output = output_layer_output - Y_train.reshape(-1, 1)\n",
    "    d_hidden = np.dot(d_output, weights_hidden_output.T) * (hidden_output * (1 - hidden_output))\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden_output -= learning_rate * np.dot(hidden_output.T, d_output)\n",
    "    bias_output -= learning_rate * np.sum(d_output, axis=0)\n",
    "    weights_input_hidden -= learning_rate * np.dot(X_train.T, d_hidden)\n",
    "    bias_hidden -= learning_rate * np.sum(d_hidden, axis=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "hidden_input = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
    "hidden_output = sigmoid(hidden_input)\n",
    "output_layer_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "# Round the output to 0 or 1 for binary classification\n",
    "predictions = np.round(output_layer_output)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(\"Testing Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
